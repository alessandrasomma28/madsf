{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d95fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6cc30",
   "metadata": {},
   "source": [
    "# Underground Alarm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e296d",
   "metadata": {},
   "source": [
    "First of all, we analyze the metrics of the disruptive scenario and compute responsiveness and fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d358a0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'passengers_new', 'passengers_departures',\n",
       "       'passengers_arrivals', 'passengers_unassigned', 'passengers_assigned',\n",
       "       'passengers_accept', 'passengers_reject', 'passengers_cancel',\n",
       "       'drivers_shift_duration_avg', 'drivers_total_length_avg',\n",
       "       'drivers_idle_duration_avg', 'drivers_occupied_distance_avg',\n",
       "       'drivers_occupied_duration_avg', 'drivers_passengers_served',\n",
       "       'drivers_idle', 'drivers_pickup', 'drivers_busy', 'drivers_accept',\n",
       "       'drivers_reject', 'drivers_removed', 'rides_in_progress',\n",
       "       'rides_waiting_duration_avg', 'rides_duration_avg', 'rides_length_avg',\n",
       "       'rides_duration_expected_avg', 'rides_length_expected_avg',\n",
       "       'rides_dispatched', 'rides_partial_acceptances', 'rides_not_served',\n",
       "       'rides_offers_generated', 'rides_offers_radius_avg',\n",
       "       'rides_offers_price_avg', 'rides_offers_surge_avg',\n",
       "       'traffic_in_progress', 'traffic_departures', 'traffic_arrivals',\n",
       "       'traffic_duration_avg', 'traffic_length_avg', 'traffic_speed_avg',\n",
       "       'traffic_speed_relative_avg', 'traffic_queuing_duration_avg',\n",
       "       'traffic_queuing_length_avg', 'rides_offers_lyft', 'rides_offers_uber',\n",
       "       'rides_offers_surge_lyft_avg', 'rides_offers_surge_uber_avg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "underground_file = os.path.join(projectPath, 'sumoenv/scenarios/underground_alarm/social_groups/21111008_21111014/sf_final_metrics.csv')\n",
    "df_under = pd.read_csv(underground_file)\n",
    "df_under.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1954ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor column in df_under.columns:\\n    if column == \"timestamp\":\\n        continue\\n    plt.figure()\\n    plt.plot(df_under[\"timestamp\"], df_under[column], label=column)\\n    plt.title(f\"Processed: {column}\")\\n    plt.xlabel(\"Timestamp\")\\n    plt.ylabel(column)\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Surge multiplier columns that shouldn't be smoothed but padded\n",
    "surge_columns = [\n",
    "    \"rides_offers_surge_avg\",\n",
    "    \"rides_offers_surge_lyft_avg\",\n",
    "    \"rides_offers_surge_uber_avg\"\n",
    "]\n",
    "# Agents columns that should be padded with forward-fill every 60 seconds before smoothing\n",
    "agent_columns = [\n",
    "    \"passengers_unassigned\",\n",
    "    \"passengers_assigned\",\n",
    "    \"passengers_accept\",\n",
    "    \"passengers_reject\",\n",
    "    \"passengers_cancel\",\n",
    "    \"drivers_idle\",\n",
    "    \"drivers_pickup\",\n",
    "    \"drivers_busy\",\n",
    "    \"drivers_accept\",\n",
    "    \"drivers_reject\",\n",
    "    \"drivers_removed\",\n",
    "    \"rides_duration_expected_avg\",\n",
    "    \"rides_length_expected_avg\",\n",
    "    \"rides_offers_radius_avg\",\n",
    "    \"rides_offers_price_avg\",\n",
    "    \"rides_dispatched\",\n",
    "    \"rides_in_progress\",\n",
    "    \"rides_offers_generated\",\n",
    "    \"rides_partial_acceptances\"\n",
    "]\n",
    "\n",
    "# Process the surge_columns: pad with forward-fill\n",
    "for col in surge_columns:\n",
    "    df_under[col] = df_under[col].replace(0, np.nan)\n",
    "    df_under[col] = df_under[col].fillna('ffill')\n",
    "\n",
    "# Process the agent_columns: pad with forward-fill\n",
    "for col in agent_columns:\n",
    "    for i in range(0, len(df_under), 60):\n",
    "        val = df_under.at[i, col]\n",
    "        df_under.loc[i:i+59, col] = val\n",
    "\n",
    "# Apply rolling mean to all other columns\n",
    "rolling_cols = [col for col in df_under.columns if col not in surge_columns + [\"timestamp\"]]\n",
    "for col in rolling_cols:\n",
    "    df_under[col] = df_under[col].rolling(window=300).mean().dropna()\n",
    "\n",
    "# Plot each column (excluding timestamp)\n",
    "'''\n",
    "for column in df_under.columns:\n",
    "    if column == \"timestamp\":\n",
    "        continue\n",
    "    plt.figure()\n",
    "    plt.plot(df_under[\"timestamp\"], df_under[column], label=column)\n",
    "    plt.title(f\"Processed: {column}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc9359",
   "metadata": {},
   "source": [
    "## Compare with Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4920c967",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'ffill'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m df_under = df_under[\u001b[32m5400\u001b[39m:-\u001b[32m3600\u001b[39m]\n\u001b[32m     53\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrides_offers_surge_avg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNormal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m plt.plot(df_under[\u001b[33m\"\u001b[39m\u001b[33mrides_offers_surge_avg\u001b[39m\u001b[33m\"\u001b[39m], label=\u001b[33m\"\u001b[39m\u001b[33mUnderground\u001b[39m\u001b[33m\"\u001b[39m, linestyle=\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     56\u001b[39m plt.title(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvg Surge Multiplier Comparison\u001b[39m\u001b[33m\"\u001b[39m, fontsize=\u001b[32m18\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/sfdigitalmirror/.venv/lib/python3.11/site-packages/matplotlib/pyplot.py:3838\u001b[39m, in \u001b[36mplot\u001b[39m\u001b[34m(scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   3830\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.plot)\n\u001b[32m   3831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m   3832\u001b[39m     *args: \u001b[38;5;28mfloat\u001b[39m | ArrayLike | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3836\u001b[39m     **kwargs,\n\u001b[32m   3837\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[32m-> \u001b[39m\u001b[32m3838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3843\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3844\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/sfdigitalmirror/.venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1779\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m-> \u001b[39m\u001b[32m1779\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scalex:\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m._request_autoscale_view(\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/sfdigitalmirror/.venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:2407\u001b[39m, in \u001b[36m_AxesBase.add_line\u001b[39m\u001b[34m(self, line)\u001b[39m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m line.get_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2405\u001b[39m     line.set_clip_path(\u001b[38;5;28mself\u001b[39m.patch)\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_line_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.get_label():\n\u001b[32m   2409\u001b[39m     line.set_label(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m_child\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._children)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/sfdigitalmirror/.venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:2430\u001b[39m, in \u001b[36m_AxesBase._update_line_limits\u001b[39m\u001b[34m(self, line)\u001b[39m\n\u001b[32m   2426\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_line_limits\u001b[39m(\u001b[38;5;28mself\u001b[39m, line):\n\u001b[32m   2427\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2428\u001b[39m \u001b[33;03m    Figures out the data limit of the given line, updating `.Axes.dataLim`.\u001b[39;00m\n\u001b[32m   2429\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2430\u001b[39m     path = \u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m path.vertices.size == \u001b[32m0\u001b[39m:\n\u001b[32m   2432\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/sfdigitalmirror/.venv/lib/python3.11/site-packages/matplotlib/lines.py:1052\u001b[39m, in \u001b[36mLine2D.get_path\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1050\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._invalidy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._invalidx:\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/sfdigitalmirror/.venv/lib/python3.11/site-packages/matplotlib/lines.py:694\u001b[39m, in \u001b[36mLine2D.recache\u001b[39m\u001b[34m(self, always)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m always \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._invalidy:\n\u001b[32m    693\u001b[39m     yconv = \u001b[38;5;28mself\u001b[39m.convert_yunits(\u001b[38;5;28mself\u001b[39m._yorig)\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m     y = \u001b[43m_to_unmasked_float_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43myconv\u001b[49m\u001b[43m)\u001b[49m.ravel()\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    696\u001b[39m     y = \u001b[38;5;28mself\u001b[39m._y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/sfdigitalmirror/.venv/lib/python3.11/site-packages/matplotlib/cbook.py:1355\u001b[39m, in \u001b[36m_to_unmasked_float_array\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   1353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.ma.asarray(x, \u001b[38;5;28mfloat\u001b[39m).filled(np.nan)\n\u001b[32m   1354\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(x, \u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'ffill'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "normal_file = os.path.join(projectPath, 'sumoenv/scenarios/normal/social_groups_2/21111008_21111014/sf_final_metrics.csv')\n",
    "df = pd.read_csv(normal_file)\n",
    "\n",
    "# Surge multiplier columns that shouldn't be smoothed but padded\n",
    "surge_columns = [\n",
    "    \"rides_offers_surge_avg\",\n",
    "    \"rides_offers_surge_lyft_avg\",\n",
    "    \"rides_offers_surge_uber_avg\"\n",
    "]\n",
    "# Agents columns that should be padded with forward-fill every 60 seconds before smoothing\n",
    "agent_columns = [\n",
    "    \"passengers_unassigned\",\n",
    "    \"passengers_assigned\",\n",
    "    \"passengers_accept\",\n",
    "    \"passengers_reject\",\n",
    "    \"passengers_cancel\",\n",
    "    \"drivers_idle\",\n",
    "    \"drivers_pickup\",\n",
    "    \"drivers_busy\",\n",
    "    \"drivers_accept\",\n",
    "    \"drivers_reject\",\n",
    "    \"drivers_removed\",\n",
    "    \"rides_duration_expected_avg\",\n",
    "    \"rides_length_expected_avg\",\n",
    "    \"rides_offers_radius_avg\",\n",
    "    \"rides_offers_price_avg\",\n",
    "    \"rides_dispatched\",\n",
    "    \"rides_in_progress\",\n",
    "    \"rides_offers_generated\",\n",
    "    \"rides_partial_acceptances\"\n",
    "]\n",
    "\n",
    "# Process the surge_columns: pad with forward-fill\n",
    "for col in surge_columns:\n",
    "    df[col] = df[col].replace(0, np.nan)\n",
    "    df[col] = df[col].fillna('ffill')\n",
    "\n",
    "# Process the agent_columns: pad with forward-fill\n",
    "for col in agent_columns:\n",
    "    for i in range(0, len(df), 60):\n",
    "        val = df.at[i, col]\n",
    "        df.loc[i:i+59, col] = val\n",
    "\n",
    "# Apply rolling mean to all other columns\n",
    "rolling_cols = [col for col in df.columns if col not in surge_columns + [\"timestamp\"]]\n",
    "for col in rolling_cols:\n",
    "    df[col] = df[col].rolling(window=300).mean().dropna()\n",
    "\n",
    "df = df[5400:-3600]\n",
    "df_under = df_under[5400:-3600]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"timestamp\"], df[\"rides_offers_surge_avg\"], label=\"Normal\")\n",
    "plt.plot(df_under[\"rides_offers_surge_avg\"], label=\"Underground\", linestyle='--')\n",
    "plt.title(f\"Avg Surge Multiplier Comparison\", fontsize=18)\n",
    "plt.xlabel(\"Timestamp\", fontsize=14)\n",
    "plt.ylabel(\"Avg Surge Multiplier Value\", fontsize=14)\n",
    "plt.ylim(0, 3.5)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/avg_surge_multiplier_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006d3f8",
   "metadata": {},
   "source": [
    "## Responsiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53329ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficiency summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/results/efficiency_underground.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare paths\n",
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir = os.path.join(projectPath, 'sumoenv/scenarios/underground_alarm/social_groups')\n",
    "\n",
    "records = []\n",
    "for date in os.listdir(mode_dir):\n",
    "    date_path = os.path.join(mode_dir, date)\n",
    "    if not os.path.isdir(date_path):\n",
    "        continue\n",
    "    summary_path = os.path.join(date_path, 'simulation_summary.csv')\n",
    "    if os.path.exists(summary_path):\n",
    "        df = pd.read_csv(summary_path)\n",
    "        if not df.empty:\n",
    "            first_row = df.iloc[0]\n",
    "            elapsed_sec = first_row['total_elapsed_seconds']\n",
    "            sumo_sec = first_row['sumo_time']\n",
    "            agents_sec = first_row['agents_time']\n",
    "            records.append({\n",
    "                'mode': 'social_groups',\n",
    "                'date': date,\n",
    "                'elapsed_seconds': elapsed_sec,\n",
    "                'elapsed_minutes': round(elapsed_sec / 60, 2),\n",
    "                'sumo_time': sumo_sec,\n",
    "                'sumo_minutes': round(sumo_sec / 60, 2),\n",
    "                'agents_time': agents_sec,\n",
    "                'agents_minutes': round(agents_sec / 60, 2)\n",
    "            })\n",
    "# Save results to CSV\n",
    "summary_df = pd.DataFrame(records)\n",
    "output_file = os.path.join(projectPath, 'experiments/results/responsiveness_underground.csv')\n",
    "summary_df = summary_df.sort_values(by=['date'])\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Responsiveness summary saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff3c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficiency ratio (underground/normal): 20.51%\n"
     ]
    }
   ],
   "source": [
    "# Compare responsiveness ratio of underground and normal scenarios\n",
    "df_under = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_underground.csv'))\n",
    "df_normal = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_normal.csv'))\n",
    "df_under_dates = df_under['date'].unique()\n",
    "df_normal = df_normal[df_normal['mode'] == 'social_groups_avg']\n",
    "df_normal = df_normal[df_normal['date'].isin(df_under_dates)]\n",
    "total_under = df_under['elapsed_seconds'].sum()\n",
    "total_normal = df_normal['elapsed_seconds'].sum()\n",
    "ratio = ((total_under - total_normal) / total_normal) * 100\n",
    "print(f\"Responsiveness ratio (underground/normal): {ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb0901",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickups_dropoffs(\n",
    "        sf_rides_stats_path,\n",
    "        start_date_str,\n",
    "        start_time_str,\n",
    "        end_date_str,\n",
    "        end_time_str,\n",
    "        tazs_involved = None\n",
    "    ):\n",
    "    start_date = datetime.strptime(start_date_str, \"%y%m%d\").date()\n",
    "    end_date = datetime.strptime(end_date_str, \"%y%m%d\").date()\n",
    "    num_days = (end_date - start_date).days + 1\n",
    "\n",
    "    # Parse start and end hours\n",
    "    start_hour = int(datetime.strptime(start_time_str, \"%H\").hour)\n",
    "    end_hour = int(datetime.strptime(end_time_str, \"%H\").hour)\n",
    "\n",
    "    # Map dataset hours (3–26) to standard 0–23 format\n",
    "    dataset_hour_map = {h: h % 24 for h in range(3, 27)}\n",
    "\n",
    "    # Read the CSV file    \n",
    "    all_rows = []\n",
    "    with open(sf_rides_stats_path, mode='r') as file:\n",
    "        reader = csv.DictReader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            row['taz'] = int(row['taz'])\n",
    "            row['day_of_week'] = int(row['day_of_week'])\n",
    "            row['hour'] = int(row['hour'])\n",
    "            row['pickups'] = round(float(row['pickups']))\n",
    "            row['dropoffs'] = round(float(row['dropoffs']))\n",
    "            all_rows.append(row)\n",
    "\n",
    "    # Index by (day_of_week, hour, taz)\n",
    "    data_by_key = {}\n",
    "    for row in all_rows:\n",
    "        key = (row['day_of_week'], row['hour'], row['taz'])\n",
    "        data_by_key[key] = {'pickups': row['pickups'], 'dropoffs': row['dropoffs']}\n",
    "\n",
    "    zone_data = {}\n",
    "    # For each simulation day, determine hours to include from that day\n",
    "    for sim_day_index in range(num_days):\n",
    "        sim_date = start_date + timedelta(days=sim_day_index)\n",
    "        sim_day_of_week = sim_date.weekday()\n",
    "        if num_days == 1:\n",
    "            selected_std_hours = list(range(start_hour, end_hour))\n",
    "        else:\n",
    "            if sim_day_index == 0:\n",
    "                selected_std_hours = list(range(start_hour, 24))\n",
    "            elif sim_day_index == num_days - 1:\n",
    "                selected_std_hours = list(range(0, end_hour))\n",
    "            else:\n",
    "                selected_std_hours = list(range(0, 24))\n",
    "        selected_dataset_hours = {h: std for h, std in dataset_hour_map.items() if std in selected_std_hours}\n",
    "        # Filter rows for this day and hour\n",
    "        for row in all_rows:\n",
    "            taz = row['taz']\n",
    "            hour = row['hour']\n",
    "            day = row['day_of_week']\n",
    "            if day == sim_day_of_week and hour in selected_dataset_hours:\n",
    "                std_hour = selected_dataset_hours[hour]\n",
    "                if taz not in zone_data:\n",
    "                    zone_data[taz] = {}\n",
    "                zone_data[taz][std_hour] = {\n",
    "                    'pickups': row['pickups'],\n",
    "                    'dropoffs': row['dropoffs']\n",
    "                }\n",
    "    \n",
    "    # If tazs_involved is provided, adjust pickups and dropoffs based on \"underground\" disruptive scenario\n",
    "    if tazs_involved:\n",
    "        for taz in zone_data:\n",
    "            if taz in tazs_involved:\n",
    "                for hour in zone_data[taz]:\n",
    "                    if hour == 23 or hour == 11:\n",
    "                        zone_data[taz][hour]['pickups'] += round((zone_data[taz][hour]['pickups'] * 2) * 0.7)\n",
    "                        zone_data[taz][hour]['dropoffs'] += round((zone_data[taz][hour]['dropoffs'] * 2) * 0.7)\n",
    "\n",
    "    # Compute pickups and dropoffs across all zones and selected hours\n",
    "    total_pickups = sum(hour_data['pickups'] for zone in zone_data.values() for hour_data in zone.values())\n",
    "    total_dropoffs = sum(hour_data['dropoffs'] for zone in zone_data.values() for hour_data in zone.values())\n",
    "\n",
    "    return total_pickups, total_dropoffs\n",
    "\n",
    "\n",
    "def percent_error(true_val, estimated_val):\n",
    "    return 100 * abs(true_val - estimated_val) / true_val if true_val != 0 else float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c60ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/results/fidelity_underground.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare paths\n",
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir = os.path.join(projectPath, 'sumoenv/scenarios/underground_alarm/social_groups')\n",
    "traffic_dir = os.path.join(projectPath, 'data/sf_traffic/sfmta_dataset')\n",
    "sfcta_dir = os.path.join(projectPath, 'data/ridehailing_stats')\n",
    "\n",
    "# Initialize records and TAZs involved\n",
    "records = []\n",
    "tazs_involved = None\n",
    "with open(os.path.join(projectPath, \"config/zip_zones_config.json\"), \"r\") as f:\n",
    "    zip_zones = json.load(f)\n",
    "    tazs_involved = []\n",
    "    with open(os.path.join(projectPath, \"data/sf_zones/sf_sfcta_stanford_mapping.json\"), \"r\") as f:\n",
    "        sfcta_mapping = json.load(f)\n",
    "    for taz in zip_zones[\"downtown\"]:\n",
    "        if taz in sfcta_mapping:\n",
    "            tazs_involved.extend(sfcta_mapping[taz])\n",
    "\n",
    "# Traverse each folder inside the mode\n",
    "for date in os.listdir(mode_dir):\n",
    "    date_path = os.path.join(mode_dir, date)\n",
    "    if not os.path.isdir(date_path):\n",
    "        continue\n",
    "    summary_path = os.path.join(date_path, 'sf_final_metrics.csv')\n",
    "    # Get real traffic data\n",
    "    traffic_file = [f for f in os.listdir(traffic_dir) if os.path.isfile(os.path.join(traffic_dir, f)) and date in f]\n",
    "    try:\n",
    "        traffic_df = pd.read_csv(os.path.join(traffic_dir, traffic_file[0]))\n",
    "        # Here we read the traffic data related to the disruptive scenario\n",
    "        od_df = pd.read_csv(os.path.join(date_path, f\"sf_traffic_od_{date}.csv\"), sep=';')\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # Filter the traffic data by hours and TAZs involved\n",
    "    if tazs_involved:\n",
    "        od_df = od_df[od_df['origin_taz_id'].isin(tazs_involved)]\n",
    "        od_df_filtered_taz = od_df.copy()\n",
    "        od_df_filtered_taz['origin_starting_time'] = pd.to_datetime(od_df_filtered_taz['origin_starting_time'])\n",
    "        time_ranges = [\n",
    "        (pd.to_datetime('11:00:00').time(), pd.to_datetime('11:40:00').time()),\n",
    "        (pd.to_datetime('23:00:00').time(), pd.to_datetime('23:40:00').time())\n",
    "        ]\n",
    "        od_df_filtered_taz_time = od_df_filtered_taz[\n",
    "            od_df_filtered_taz['origin_starting_time'].dt.time.between(*time_ranges[0]) |\n",
    "            od_df_filtered_taz['origin_starting_time'].dt.time.between(*time_ranges[1])\n",
    "        ]\n",
    "        # Consider +50% traffic\n",
    "        base_count = len(od_df_filtered_taz_time) * 0.64\n",
    "        added_count = int(base_count * 0.5)\n",
    "        new_count = base_count + added_count\n",
    "    else:\n",
    "        new_count = 0\n",
    "    traffic = (len(traffic_df)-1) if traffic_file else 0\n",
    "    # Scale traffic: 36% of traffic is TNC\n",
    "    traffic = traffic + new_count\n",
    "    traffic_scaled = int(traffic * 0.64)\n",
    "\n",
    "    # Get pickups and dropoffs data\n",
    "    start_str, end_str = date.split('_')\n",
    "    pickups, dropoffs = get_pickups_dropoffs(\n",
    "        os.path.join(sfcta_dir, \"trip_stats_taz.csv\"),\n",
    "        start_str[:6],\n",
    "        start_str[6:],\n",
    "        end_str[:6],\n",
    "        end_str[6:],\n",
    "        tazs_involved\n",
    "    )\n",
    "\n",
    "    # Recotrd the results\n",
    "    if os.path.exists(summary_path):\n",
    "        df = pd.read_csv(summary_path)\n",
    "        if not df.empty:\n",
    "            traffic_count = sum(df['traffic_departures'])\n",
    "            pickup_count = sum(df['passengers_departures'])\n",
    "            dropoff_count = sum(df['passengers_arrivals'])\n",
    "            canceled_count = sum(df['passengers_cancel'])\n",
    "            traffic_error = percent_error(traffic_scaled, traffic_count)\n",
    "            pickup_error = percent_error(pickups, pickup_count)\n",
    "            dropoff_error = percent_error(dropoffs, dropoff_count)\n",
    "            pickup_scaled_error = percent_error(pickups, pickup_count + canceled_count)\n",
    "            dropoff_scaled_error = percent_error(dropoffs, dropoff_count + canceled_count)\n",
    "            records.append({\n",
    "                'mode': 'social_groups',\n",
    "                'date': date,\n",
    "                'traffic_input': traffic_scaled,\n",
    "                'pickup_input': pickups,\n",
    "                'dropoff_input': dropoffs,\n",
    "                'traffic_output': traffic_count,\n",
    "                'pickup_output': pickup_count,\n",
    "                'dropoff_output': dropoff_count,\n",
    "                'traffic_error': round(traffic_error, 2),\n",
    "                'pickup_error': round(pickup_error, 2),\n",
    "                'dropoff_error': round(dropoff_error, 2),\n",
    "                'canceled_rides': canceled_count,\n",
    "                'pickup_scaled_error': round(pickup_scaled_error, 2),\n",
    "                'dropoff_scaled_error': round(dropoff_scaled_error, 2)\n",
    "            })\n",
    "\n",
    "# Save results to CSV\n",
    "summary_df = pd.DataFrame(records)\n",
    "output_file = os.path.join(projectPath, 'experiments/results/fidelity_underground.csv')\n",
    "summary_df = summary_df.sort_values(by=['date'])\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Fidelity summary saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ea71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic fidelity ratio (underground/normal): 2.81%\n",
      "Pickup fidelity ratio (underground/normal): -0.26%\n",
      "Dropoff fidelity ratio (underground/normal): -0.19%\n",
      "Canceled rides ratio (underground/normal): 141.84%\n"
     ]
    }
   ],
   "source": [
    "# Compare fidelity of underground and normal scenarios\n",
    "df_under = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_underground.csv'))\n",
    "df_normal = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_normal.csv'))\n",
    "df_under_dates = df_under['date'].unique()\n",
    "df_normal = df_normal[df_normal['mode'] == 'social_groups_avg']\n",
    "df_normal = df_normal[df_normal['date'].isin(df_under_dates)]\n",
    "under_traffic_error = df_under['traffic_error'].mean()\n",
    "normal_traffic_error = df_normal['traffic_error'].mean()\n",
    "under_pickup_error = 100 - df_under['pickup_scaled_error'].mean()\n",
    "normal_pickup_error = 100 - df_normal['pickup_scaled_error'].mean()\n",
    "under_dropoff_error = 100 - df_under['dropoff_scaled_error'].mean()\n",
    "normal_dropoff_error = 100 - df_normal['dropoff_scaled_error'].mean()\n",
    "under_rides_canceled = df_under['canceled_rides'].mean()\n",
    "normal_rides_canceled = df_normal['canceled_rides'].mean()\n",
    "traffic_ratio = ((under_traffic_error - normal_traffic_error) / normal_traffic_error) * 100\n",
    "pickup_ratio = ((under_pickup_error - normal_pickup_error) / normal_pickup_error) * 100\n",
    "dropoff_ratio = ((under_dropoff_error - normal_dropoff_error) / normal_dropoff_error) * 100\n",
    "canceled_ratio = ((under_rides_canceled - normal_rides_canceled) / normal_rides_canceled) * 100\n",
    "print(f\"Traffic fidelity ratio (underground/normal): {traffic_ratio:.2f}%\")\n",
    "print(f\"Pickup fidelity ratio (underground/normal): {pickup_ratio:.2f}%\")\n",
    "print(f\"Dropoff fidelity ratio (underground/normal): {dropoff_ratio:.2f}%\")\n",
    "print(f\"Canceled rides ratio (underground/normal): {canceled_ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77804f76",
   "metadata": {},
   "source": [
    "# Flat Provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc7cbe",
   "metadata": {},
   "source": [
    "## Responsiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d05f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficiency summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/results/efficiency_normal_new_prov.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare paths\n",
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir = os.path.join(projectPath, 'sumoenv/scenarios/normal_new_prov/social_groups')\n",
    "\n",
    "records = []\n",
    "for date in os.listdir(mode_dir):\n",
    "    date_path = os.path.join(mode_dir, date)\n",
    "    if not os.path.isdir(date_path):\n",
    "        continue\n",
    "    summary_path = os.path.join(date_path, 'simulation_summary.csv')\n",
    "    if os.path.exists(summary_path):\n",
    "        df = pd.read_csv(summary_path)\n",
    "        if not df.empty:\n",
    "            first_row = df.iloc[0]\n",
    "            elapsed_sec = first_row['total_elapsed_seconds']\n",
    "            sumo_sec = first_row['sumo_time']\n",
    "            agents_sec = first_row['agents_time']\n",
    "            records.append({\n",
    "                'mode': 'social_groups',\n",
    "                'date': date,\n",
    "                'elapsed_seconds': elapsed_sec,\n",
    "                'elapsed_minutes': round(elapsed_sec / 60, 2),\n",
    "                'sumo_time': sumo_sec,\n",
    "                'sumo_minutes': round(sumo_sec / 60, 2),\n",
    "                'agents_time': agents_sec,\n",
    "                'agents_minutes': round(agents_sec / 60, 2)\n",
    "            })\n",
    "# Save results to CSV\n",
    "summary_df = pd.DataFrame(records)\n",
    "output_file = os.path.join(projectPath, 'experiments/results/responsiveness_normal_new_prov.csv')\n",
    "summary_df = summary_df.sort_values(by=['date'])\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Responsiveness summary saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed seconds (flat): 256443.28000000003\n",
      "Total elapsed seconds (normal): 127460.29000000001\n",
      "Efficiency ratio (flat/normal): 50.30%\n"
     ]
    }
   ],
   "source": [
    "# Compare responsiveness ratio of new provider and standard config\n",
    "df_flat = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_normal_new_prov.csv'))\n",
    "df_normal = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_normal.csv'))\n",
    "df_normal = df_normal[df_normal['mode'] == 'social_groups_avg']\n",
    "total_flat = df_flat['elapsed_seconds'].sum()\n",
    "total_normal = df_normal['elapsed_seconds'].sum()\n",
    "print(f\"Total elapsed seconds (flat): {total_flat}\")\n",
    "print(f\"Total elapsed seconds (normal): {total_normal}\")\n",
    "ratio = ((total_flat - total_normal) / total_flat) * 100\n",
    "print(f\"Responsiveness ratio (flat/normal): {ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa80d0",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ddabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/results/fidelity_normal_new_prov.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare paths\n",
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir = os.path.join(projectPath, 'sumoenv/scenarios/normal_new_prov/social_groups')\n",
    "traffic_dir = os.path.join(projectPath, 'data/sf_traffic/sfmta_dataset')\n",
    "sfcta_dir = os.path.join(projectPath, 'data/ridehailing_stats')\n",
    "\n",
    "records = []\n",
    "# Traverse each folder inside the mode\n",
    "for date in os.listdir(mode_dir):\n",
    "    date_path = os.path.join(mode_dir, date)\n",
    "    if not os.path.isdir(date_path):\n",
    "        continue\n",
    "    summary_path = os.path.join(date_path, 'sf_final_metrics.csv')\n",
    "    # Get real traffic and pickup/dropoff data\n",
    "    traffic_file = [f for f in os.listdir(traffic_dir) if os.path.isfile(os.path.join(traffic_dir, f)) and date in f]\n",
    "    try:\n",
    "        traffic_df = pd.read_csv(os.path.join(traffic_dir, traffic_file[0]))\n",
    "    except:\n",
    "        continue\n",
    "    traffic = (len(traffic_df)-1) if traffic_file else 0\n",
    "    # Scale traffic: 36% of traffic is TNC\n",
    "    traffic_scaled = int(traffic * 0.64)\n",
    "    start_str, end_str = date.split('_')\n",
    "    pickups, dropoffs = get_pickups_dropoffs(\n",
    "        os.path.join(sfcta_dir, \"trip_stats_taz.csv\"),\n",
    "        start_str[:6],\n",
    "        start_str[6:],\n",
    "        end_str[:6],\n",
    "        end_str[6:]\n",
    "    )\n",
    "    if os.path.exists(summary_path):\n",
    "        df = pd.read_csv(summary_path)\n",
    "        if not df.empty:\n",
    "            traffic_count = sum(df['traffic_departures'])\n",
    "            pickup_count = sum(df['passengers_departures'])\n",
    "            dropoff_count = sum(df['passengers_arrivals'])\n",
    "            canceled_count = sum(df['passengers_cancel']) + sum(df['rides_not_served'])\n",
    "            traffic_error = percent_error(traffic_scaled, traffic_count)\n",
    "            pickup_error = percent_error(pickups, pickup_count)\n",
    "            dropoff_error = percent_error(dropoffs, dropoff_count)\n",
    "            pickup_scaled_error = percent_error(pickups, pickup_count + canceled_count)\n",
    "            dropoff_scaled_error = percent_error(dropoffs, dropoff_count + canceled_count)\n",
    "            records.append({\n",
    "                'mode': 'social_groups',\n",
    "                'date': date,\n",
    "                'traffic_input': traffic_scaled,\n",
    "                'pickup_input': pickups,\n",
    "                'dropoff_input': dropoffs,\n",
    "                'traffic_output': traffic_count,\n",
    "                'pickup_output': pickup_count,\n",
    "                'dropoff_output': dropoff_count,\n",
    "                'traffic_error': round(traffic_error, 2),\n",
    "                'pickup_error': round(pickup_error, 2),\n",
    "                'dropoff_error': round(dropoff_error, 2),\n",
    "                'canceled_rides': canceled_count,\n",
    "                'pickup_scaled_error': round(pickup_scaled_error, 2),\n",
    "                'dropoff_scaled_error': round(dropoff_scaled_error, 2)\n",
    "            })\n",
    "\n",
    "# Save results to CSV\n",
    "summary_df = pd.DataFrame(records)\n",
    "output_file = os.path.join(projectPath, 'experiments/results/fidelity_normal_new_prov.csv')\n",
    "summary_df = summary_df.sort_values(by=['date'])\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Fidelity summary saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf74c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6451.708333333333\n",
      "3404.722222222222\n",
      "Traffic fidelity ratio (flat/normal): -3.76%\n",
      "Pickup fidelity ratio (flat/normal): 0.07%\n",
      "Dropoff fidelity ratio (flat/normal): 0.05%\n",
      "Canceled rides ratio (flat/normal): 89.49%\n"
     ]
    }
   ],
   "source": [
    "# Compare fidelity of new provider and standard config\n",
    "df_flat = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_normal_new_prov.csv'))\n",
    "df_normal = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_normal.csv'))\n",
    "df_normal = df_normal[df_normal['mode'] == 'social_groups_avg']\n",
    "flat_traffic_error = df_flat['traffic_error'].mean()\n",
    "normal_traffic_error = df_normal['traffic_error'].mean()\n",
    "flat_pickup_error = 100 - df_flat['pickup_scaled_error'].mean()\n",
    "normal_pickup_error = 100 - df_normal['pickup_scaled_error'].mean()\n",
    "flat_dropoff_error = 100 - df_flat['dropoff_scaled_error'].mean()\n",
    "normal_dropoff_error = 100 - df_normal['dropoff_scaled_error'].mean()\n",
    "flat_rides_canceled = df_flat['canceled_rides'].mean()\n",
    "normal_rides_canceled = df_normal['canceled_rides'].mean()\n",
    "traffic_ratio = ((flat_traffic_error - normal_traffic_error) / normal_traffic_error) * 100\n",
    "pickup_ratio = ((flat_pickup_error - normal_pickup_error) / normal_pickup_error) * 100\n",
    "dropoff_ratio = ((flat_dropoff_error - normal_dropoff_error) / normal_dropoff_error) * 100\n",
    "canceled_ratio = ((flat_rides_canceled - normal_rides_canceled) / normal_rides_canceled) * 100\n",
    "print(f\"Traffic fidelity ratio (flat/normal): {traffic_ratio:.2f}%\")\n",
    "print(f\"Pickup fidelity ratio (flat/normal): {pickup_ratio:.2f}%\")\n",
    "print(f\"Dropoff fidelity ratio (flat/normal): {dropoff_ratio:.2f}%\")\n",
    "print(f\"Canceled rides ratio (flat/normal): {canceled_ratio:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
