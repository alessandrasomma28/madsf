{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6cc30",
   "metadata": {},
   "source": [
    "# Underground Alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "underground_file = os.path.join(projectPath, 'sumoenv/scenarios/underground_alarm/social_groups/21111008_21111014/sf_final_metrics.csv')\n",
    "df_under = pd.read_csv(underground_file)\n",
    "df_under.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1954ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surge multiplier columns that shouldn't be smoothed but padded\n",
    "surge_columns = [\n",
    "    \"rides_offers_surge_avg\",\n",
    "    \"rides_offers_surge_lyft_avg\",\n",
    "    \"rides_offers_surge_uber_avg\"\n",
    "]\n",
    "# Agents columns that should be padded with forward-fill every 60 seconds before smoothing\n",
    "agent_columns = [\n",
    "    \"passengers_unassigned\",\n",
    "    \"passengers_assigned\",\n",
    "    \"passengers_accept\",\n",
    "    \"passengers_reject\",\n",
    "    \"passengers_cancel\",\n",
    "    \"drivers_idle\",\n",
    "    \"drivers_pickup\",\n",
    "    \"drivers_busy\",\n",
    "    \"drivers_accept\",\n",
    "    \"drivers_reject\",\n",
    "    \"drivers_removed\",\n",
    "    \"rides_duration_expected_avg\",\n",
    "    \"rides_length_expected_avg\",\n",
    "    \"rides_offers_radius_avg\",\n",
    "    \"rides_offers_price_avg\",\n",
    "    \"rides_dispatched\",\n",
    "    \"rides_in_progress\",\n",
    "    \"rides_offers_generated\",\n",
    "    \"rides_partial_acceptances\"\n",
    "]\n",
    "\n",
    "# Process the surge_columns: pad with forward-fill\n",
    "for col in surge_columns:\n",
    "    df_under[col] = df_under[col].replace(0, np.nan)\n",
    "    df_under[col] = df_under[col].fillna(method='ffill')\n",
    "\n",
    "# Process the agent_columns: pad with forward-fill\n",
    "for col in agent_columns:\n",
    "    for i in range(0, len(df_under), 60):\n",
    "        val = df_under.at[i, col]\n",
    "        df_under.loc[i:i+59, col] = val\n",
    "\n",
    "# Apply rolling mean to all other columns\n",
    "rolling_cols = [col for col in df_under.columns if col not in surge_columns + [\"timestamp\"]]\n",
    "for col in rolling_cols:\n",
    "    df_under[col] = df_under[col].rolling(window=300).mean().dropna()\n",
    "\n",
    "# Plot each column (excluding timestamp)\n",
    "'''\n",
    "for column in df_under.columns:\n",
    "    if column == \"timestamp\":\n",
    "        continue\n",
    "    plt.figure()\n",
    "    plt.plot(df_under[\"timestamp\"], df_under[column], label=column)\n",
    "    plt.title(f\"Processed: {column}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc9359",
   "metadata": {},
   "source": [
    "## Compare with normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "normal_file = os.path.join(projectPath, 'sumoenv/scenarios/normal/social_groups/21111008_21111014/sf_final_metrics.csv')\n",
    "df = pd.read_csv(normal_file)\n",
    "\n",
    "# Surge multiplier columns that shouldn't be smoothed but padded\n",
    "surge_columns = [\n",
    "    \"rides_offers_surge_avg\",\n",
    "    \"rides_offers_surge_lyft_avg\",\n",
    "    \"rides_offers_surge_uber_avg\"\n",
    "]\n",
    "# Agents columns that should be padded with forward-fill every 60 seconds before smoothing\n",
    "agent_columns = [\n",
    "    \"passengers_unassigned\",\n",
    "    \"passengers_assigned\",\n",
    "    \"passengers_accept\",\n",
    "    \"passengers_reject\",\n",
    "    \"passengers_cancel\",\n",
    "    \"drivers_idle\",\n",
    "    \"drivers_pickup\",\n",
    "    \"drivers_busy\",\n",
    "    \"drivers_accept\",\n",
    "    \"drivers_reject\",\n",
    "    \"drivers_removed\",\n",
    "    \"rides_duration_expected_avg\",\n",
    "    \"rides_length_expected_avg\",\n",
    "    \"rides_offers_radius_avg\",\n",
    "    \"rides_offers_price_avg\",\n",
    "    \"rides_dispatched\",\n",
    "    \"rides_in_progress\",\n",
    "    \"rides_offers_generated\",\n",
    "    \"rides_partial_acceptances\"\n",
    "]\n",
    "\n",
    "# Process the surge_columns: pad with forward-fill\n",
    "for col in surge_columns:\n",
    "    df[col] = df[col].replace(0, np.nan)\n",
    "    df[col] = df[col].fillna(method='ffill')\n",
    "\n",
    "# Process the agent_columns: pad with forward-fill\n",
    "for col in agent_columns:\n",
    "    for i in range(0, len(df), 60):\n",
    "        val = df.at[i, col]\n",
    "        df.loc[i:i+59, col] = val\n",
    "\n",
    "# Apply rolling mean to all other columns\n",
    "rolling_cols = [col for col in df.columns if col not in surge_columns + [\"timestamp\"]]\n",
    "for col in rolling_cols:\n",
    "    df[col] = df[col].rolling(window=300).mean().dropna()\n",
    "\n",
    "df = df[5400:-3600]\n",
    "df_under = df_under[5400:-3600]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"timestamp\"], df[\"rides_offers_surge_avg\"], label=\"Normal\")\n",
    "plt.plot(df_under[\"rides_offers_surge_avg\"], label=\"Underground\", linestyle='--')\n",
    "plt.title(f\"Avg Surge Multiplier Comparison\", fontsize=18)\n",
    "plt.xlabel(\"Timestamp\", fontsize=14)\n",
    "plt.ylabel(\"Avg Surge Multiplier Value\", fontsize=14)\n",
    "plt.ylim(0, 3.5)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"avg_surge_multiplier_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006d3f8",
   "metadata": {},
   "source": [
    "## Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53329ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare paths\n",
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir = os.path.join(projectPath, 'sumoenv/scenarios/underground_alarm/social_groups')\n",
    "\n",
    "records = []\n",
    "for date in os.listdir(mode_dir):\n",
    "    date_path = os.path.join(mode_dir, date)\n",
    "    if not os.path.isdir(date_path):\n",
    "        continue\n",
    "    summary_path = os.path.join(date_path, 'simulation_summary.csv')\n",
    "    if os.path.exists(summary_path):\n",
    "        df = pd.read_csv(summary_path)\n",
    "        if not df.empty:\n",
    "            first_row = df.iloc[0]\n",
    "            elapsed_sec = first_row['total_elapsed_seconds']\n",
    "            sumo_sec = first_row['sumo_time']\n",
    "            agents_sec = first_row['agents_time']\n",
    "            records.append({\n",
    "                'mode': 'social_groups',\n",
    "                'date': date,\n",
    "                'elapsed_seconds': elapsed_sec,\n",
    "                'elapsed_minutes': round(elapsed_sec / 60, 2),\n",
    "                'sumo_time': sumo_sec,\n",
    "                'sumo_minutes': round(sumo_sec / 60, 2),\n",
    "                'agents_time': agents_sec,\n",
    "                'agents_minutes': round(agents_sec / 60, 2)\n",
    "            })\n",
    "# Save results to CSV\n",
    "summary_df = pd.DataFrame(records)\n",
    "output_file = os.path.join(projectPath, 'experiments/efficiency_results_underground.csv')\n",
    "summary_df = summary_df.sort_values(by=['date'])\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Efficiency summary saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb0901",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickups_dropoffs(\n",
    "        sf_rides_stats_path,\n",
    "        start_date_str,\n",
    "        start_time_str,\n",
    "        end_date_str,\n",
    "        end_time_str\n",
    "    ):\n",
    "    start_date = datetime.strptime(start_date_str, \"%y%m%d\").date()\n",
    "    end_date = datetime.strptime(end_date_str, \"%y%m%d\").date()\n",
    "    num_days = (end_date - start_date).days + 1\n",
    "\n",
    "    # Parse start and end hours\n",
    "    start_hour = int(datetime.strptime(start_time_str, \"%H\").hour)\n",
    "    end_hour = int(datetime.strptime(end_time_str, \"%H\").hour)\n",
    "\n",
    "    # Map dataset hours (3–26) to standard 0–23 format\n",
    "    dataset_hour_map = {h: h % 24 for h in range(3, 27)}\n",
    "\n",
    "    # Read the CSV file    \n",
    "    all_rows = []\n",
    "    with open(sf_rides_stats_path, mode='r') as file:\n",
    "        reader = csv.DictReader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            row['taz'] = int(row['taz'])\n",
    "            row['day_of_week'] = int(row['day_of_week'])\n",
    "            row['hour'] = int(row['hour'])\n",
    "            row['pickups'] = round(float(row['pickups']))\n",
    "            row['dropoffs'] = round(float(row['dropoffs']))\n",
    "            all_rows.append(row)\n",
    "\n",
    "    # Index by (day_of_week, hour, taz)\n",
    "    data_by_key = {}\n",
    "    for row in all_rows:\n",
    "        key = (row['day_of_week'], row['hour'], row['taz'])\n",
    "        data_by_key[key] = {'pickups': row['pickups'], 'dropoffs': row['dropoffs']}\n",
    "\n",
    "    zone_data = {}\n",
    "    # For each simulation day, determine hours to include from that day\n",
    "    for sim_day_index in range(num_days):\n",
    "        sim_date = start_date + timedelta(days=sim_day_index)\n",
    "        sim_day_of_week = sim_date.weekday()\n",
    "        if num_days == 1:\n",
    "            selected_std_hours = list(range(start_hour, end_hour))\n",
    "        else:\n",
    "            if sim_day_index == 0:\n",
    "                selected_std_hours = list(range(start_hour, 24))\n",
    "            elif sim_day_index == num_days - 1:\n",
    "                selected_std_hours = list(range(0, end_hour))\n",
    "            else:\n",
    "                selected_std_hours = list(range(0, 24))\n",
    "        selected_dataset_hours = {h: std for h, std in dataset_hour_map.items() if std in selected_std_hours}\n",
    "        # Filter rows for this day and hour\n",
    "        for row in all_rows:\n",
    "            taz = row['taz']\n",
    "            hour = row['hour']\n",
    "            day = row['day_of_week']\n",
    "            if day == sim_day_of_week and hour in selected_dataset_hours:\n",
    "                std_hour = selected_dataset_hours[hour]\n",
    "                if taz not in zone_data:\n",
    "                    zone_data[taz] = {}\n",
    "                zone_data[taz][std_hour] = {\n",
    "                    'pickups': row['pickups'],\n",
    "                    'dropoffs': row['dropoffs']\n",
    "                }\n",
    "\n",
    "    # Compute pickups and dropoffs across all zones and selected hours\n",
    "    total_pickups = sum(hour_data['pickups'] for zone in zone_data.values() for hour_data in zone.values())\n",
    "    total_dropoffs = sum(hour_data['dropoffs'] for zone in zone_data.values() for hour_data in zone.values())\n",
    "\n",
    "    return total_pickups, total_dropoffs\n",
    "\n",
    "\n",
    "def percent_error(true_val, estimated_val):\n",
    "    return 100 * abs(true_val - estimated_val) / true_val if true_val != 0 else float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c60ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare paths\n",
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir = os.path.join(projectPath, 'sumoenv/scenarios/underground_alarm/social_groups')\n",
    "traffic_dir = os.path.join(projectPath, 'data/sf_traffic/sfmta_dataset')\n",
    "sfcta_dir = os.path.join(projectPath, 'data/ridehailing_stats')\n",
    "\n",
    "records = []\n",
    "# Traverse each folder inside the mode\n",
    "for date in os.listdir(mode_dir):\n",
    "    date_path = os.path.join(mode_dir, date)\n",
    "    if not os.path.isdir(date_path):\n",
    "        continue\n",
    "    summary_path = os.path.join(date_path, 'sf_final_metrics.csv')\n",
    "    # Get real traffic and pickup/dropoff data\n",
    "    traffic_file = [f for f in os.listdir(traffic_dir) if os.path.isfile(os.path.join(traffic_dir, f)) and date in f]\n",
    "    print(traffic_file)\n",
    "    try:\n",
    "        traffic_df = pd.read_csv(os.path.join(traffic_dir, traffic_file[0]))\n",
    "    except:\n",
    "        continue\n",
    "    traffic = (len(traffic_df)-1) if traffic_file else 0\n",
    "    # Scale traffic: 36% of traffic is TNC\n",
    "    traffic_scaled = int(traffic * 0.64)\n",
    "    start_str, end_str = date.split('_')\n",
    "    pickups, dropoffs = get_pickups_dropoffs(\n",
    "        os.path.join(sfcta_dir, \"trip_stats_taz.csv\"),\n",
    "        start_str[:6],\n",
    "        start_str[6:],\n",
    "        end_str[:6],\n",
    "        end_str[6:]\n",
    "    )\n",
    "    if os.path.exists(summary_path):\n",
    "        df = pd.read_csv(summary_path)\n",
    "        if not df.empty:\n",
    "            traffic_count = sum(df['traffic_departures'])\n",
    "            pickup_count = sum(df['passengers_departures'])\n",
    "            dropoff_count = sum(df['passengers_arrivals'])\n",
    "            canceled_count = sum(df['passengers_cancel'])\n",
    "            traffic_error = percent_error(traffic_scaled, traffic_count)\n",
    "            pickup_error = percent_error(pickups, pickup_count)\n",
    "            dropoff_error = percent_error(dropoffs, dropoff_count)\n",
    "            pickup_scaled_error = percent_error(pickups, pickup_count + canceled_count)\n",
    "            dropoff_scaled_error = percent_error(dropoffs, dropoff_count + canceled_count)\n",
    "            records.append({\n",
    "                'mode': 'social_groups',\n",
    "                'date': date,\n",
    "                'traffic_input': traffic_scaled,\n",
    "                'pickup_input': pickups,\n",
    "                'dropoff_input': dropoffs,\n",
    "                'traffic_output': traffic_count,\n",
    "                'pickup_output': pickup_count,\n",
    "                'dropoff_output': dropoff_count,\n",
    "                'traffic_error': round(traffic_error, 2),\n",
    "                'pickup_error': round(pickup_error, 2),\n",
    "                'dropoff_error': round(dropoff_error, 2),\n",
    "                'canceled_rides': canceled_count,\n",
    "                'pickup_scaled_error': round(pickup_scaled_error, 2),\n",
    "                'dropoff_scaled_error': round(dropoff_scaled_error, 2)\n",
    "            })\n",
    "\n",
    "# Save results to CSV\n",
    "summary_df = pd.DataFrame(records)\n",
    "output_file = os.path.join(projectPath, 'experiments/fidelity_results_underground.csv')\n",
    "summary_df = summary_df.sort_values(by=['date'])\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Fidelity summary saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77804f76",
   "metadata": {},
   "source": [
    "# Flat Provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc7cbe",
   "metadata": {},
   "source": [
    "## Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d05f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare paths\n",
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir = os.path.join(projectPath, 'sumoenv/scenarios/normal_new_prov/social_groups')\n",
    "\n",
    "records = []\n",
    "for date in os.listdir(mode_dir):\n",
    "    date_path = os.path.join(mode_dir, date)\n",
    "    if not os.path.isdir(date_path):\n",
    "        continue\n",
    "    summary_path = os.path.join(date_path, 'simulation_summary.csv')\n",
    "    if os.path.exists(summary_path):\n",
    "        df = pd.read_csv(summary_path)\n",
    "        if not df.empty:\n",
    "            first_row = df.iloc[0]\n",
    "            elapsed_sec = first_row['total_elapsed_seconds']\n",
    "            sumo_sec = first_row['sumo_time']\n",
    "            agents_sec = first_row['agents_time']\n",
    "            records.append({\n",
    "                'mode': 'social_groups',\n",
    "                'date': date,\n",
    "                'elapsed_seconds': elapsed_sec,\n",
    "                'elapsed_minutes': round(elapsed_sec / 60, 2),\n",
    "                'sumo_time': sumo_sec,\n",
    "                'sumo_minutes': round(sumo_sec / 60, 2),\n",
    "                'agents_time': agents_sec,\n",
    "                'agents_minutes': round(agents_sec / 60, 2)\n",
    "            })\n",
    "# Save results to CSV\n",
    "summary_df = pd.DataFrame(records)\n",
    "output_file = os.path.join(projectPath, 'experiments/efficiency_results_normal_new_prov.csv')\n",
    "summary_df = summary_df.sort_values(by=['date'])\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Efficiency summary saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa80d0",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ddabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare paths\n",
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir = os.path.join(projectPath, 'sumoenv/scenarios/normal_new_prov/social_groups')\n",
    "traffic_dir = os.path.join(projectPath, 'data/sf_traffic/sfmta_dataset')\n",
    "sfcta_dir = os.path.join(projectPath, 'data/ridehailing_stats')\n",
    "\n",
    "records = []\n",
    "# Traverse each folder inside the mode\n",
    "for date in os.listdir(mode_dir):\n",
    "    date_path = os.path.join(mode_dir, date)\n",
    "    if not os.path.isdir(date_path):\n",
    "        continue\n",
    "    summary_path = os.path.join(date_path, 'sf_final_metrics.csv')\n",
    "    # Get real traffic and pickup/dropoff data\n",
    "    traffic_file = [f for f in os.listdir(traffic_dir) if os.path.isfile(os.path.join(traffic_dir, f)) and date in f]\n",
    "    print(traffic_file)\n",
    "    try:\n",
    "        traffic_df = pd.read_csv(os.path.join(traffic_dir, traffic_file[0]))\n",
    "    except:\n",
    "        continue\n",
    "    traffic = (len(traffic_df)-1) if traffic_file else 0\n",
    "    # Scale traffic: 36% of traffic is TNC\n",
    "    traffic_scaled = int(traffic * 0.64)\n",
    "    start_str, end_str = date.split('_')\n",
    "    pickups, dropoffs = get_pickups_dropoffs(\n",
    "        os.path.join(sfcta_dir, \"trip_stats_taz.csv\"),\n",
    "        start_str[:6],\n",
    "        start_str[6:],\n",
    "        end_str[:6],\n",
    "        end_str[6:]\n",
    "    )\n",
    "    if os.path.exists(summary_path):\n",
    "        df = pd.read_csv(summary_path)\n",
    "        if not df.empty:\n",
    "            traffic_count = sum(df['traffic_departures'])\n",
    "            pickup_count = sum(df['passengers_departures'])\n",
    "            dropoff_count = sum(df['passengers_arrivals'])\n",
    "            canceled_count = sum(df['passengers_cancel']) + sum(df['rides_not_served'])\n",
    "            traffic_error = percent_error(traffic_scaled, traffic_count)\n",
    "            pickup_error = percent_error(pickups, pickup_count)\n",
    "            dropoff_error = percent_error(dropoffs, dropoff_count)\n",
    "            pickup_scaled_error = percent_error(pickups, pickup_count + canceled_count)\n",
    "            dropoff_scaled_error = percent_error(dropoffs, dropoff_count + canceled_count)\n",
    "            records.append({\n",
    "                'mode': 'social_groups',\n",
    "                'date': date,\n",
    "                'traffic_input': traffic_scaled,\n",
    "                'pickup_input': pickups,\n",
    "                'dropoff_input': dropoffs,\n",
    "                'traffic_output': traffic_count,\n",
    "                'pickup_output': pickup_count,\n",
    "                'dropoff_output': dropoff_count,\n",
    "                'traffic_error': round(traffic_error, 2),\n",
    "                'pickup_error': round(pickup_error, 2),\n",
    "                'dropoff_error': round(dropoff_error, 2),\n",
    "                'canceled_rides': canceled_count,\n",
    "                'pickup_scaled_error': round(pickup_scaled_error, 2),\n",
    "                'dropoff_scaled_error': round(dropoff_scaled_error, 2)\n",
    "            })\n",
    "\n",
    "# Save results to CSV\n",
    "summary_df = pd.DataFrame(records)\n",
    "output_file = os.path.join(projectPath, 'experiments/fidelity_results_normal_new_prov.csv')\n",
    "summary_df = summary_df.sort_values(by=['date'])\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Fidelity summary saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
