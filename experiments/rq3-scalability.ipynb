{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d95fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055e4dc",
   "metadata": {},
   "source": [
    "# Stress Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6cc30",
   "metadata": {},
   "source": [
    "## Responsiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d358a0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responsiveness summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/results/responsiveness_stress_1.csv\n",
      "Responsiveness summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/results/responsiveness_stress_2.csv\n",
      "Responsiveness summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/results/responsiveness_stress_3.csv\n"
     ]
    }
   ],
   "source": [
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir_1 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_1/social_groups')\n",
    "mode_dir_2 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_2/social_groups')\n",
    "mode_dir_3 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_3/social_groups')\n",
    "output_name_1 = 'responsiveness_stress_1'\n",
    "output_name_2 = 'responsiveness_stress_2'\n",
    "output_name_3 = 'responsiveness_stress_3'\n",
    "\n",
    "def compute_responsiveness_stress_test(projectPath, mode_dir, output_name):\n",
    "    records = []\n",
    "    # Traverse each folder inside the mode\n",
    "    for date in os.listdir(mode_dir):\n",
    "        date_path = os.path.join(mode_dir, date)\n",
    "        if not os.path.isdir(date_path):\n",
    "            continue\n",
    "        summary_path = os.path.join(date_path, 'simulation_summary.csv')\n",
    "        if os.path.exists(summary_path):\n",
    "            df_summary = pd.read_csv(summary_path)\n",
    "            if not df_summary.empty:\n",
    "                # Get time metrics\n",
    "                first_row = df_summary.iloc[0]\n",
    "                elapsed_sec = first_row['total_elapsed_seconds']\n",
    "                sumo_sec = first_row['sumo_time']\n",
    "                agents_sec = first_row['agents_time']\n",
    "                metrics_path = os.path.join(date_path, 'sf_final_metrics.csv')\n",
    "                if os.path.exists(metrics_path):\n",
    "                    df_metrics = pd.read_csv(metrics_path)\n",
    "                    if not df_metrics.empty:\n",
    "                        records.append({\n",
    "                            'mode': 'social_groups',\n",
    "                            'date': date,\n",
    "                            'elapsed_seconds': elapsed_sec,\n",
    "                            'elapsed_minutes': round(elapsed_sec / 60, 2),\n",
    "                            'sumo_time': sumo_sec,\n",
    "                            'sumo_minutes': round(sumo_sec / 60, 2),\n",
    "                            'agents_time': agents_sec,\n",
    "                            'agents_minutes': round(agents_sec / 60, 2),\n",
    "                            'sumo_perc_time': round(sumo_sec / elapsed_sec, 2),\n",
    "                            'agents_perc_time': round(agents_sec / elapsed_sec, 2) if agents_sec > 0 else 0\n",
    "                        })\n",
    "                        \n",
    "    summary_df = pd.DataFrame(records)\n",
    "    output_file = os.path.join(projectPath, f'experiments/results/{output_name}.csv')\n",
    "    summary_df = summary_df.sort_values(by=['date'])\n",
    "    summary_df.to_csv(output_file, index=False)\n",
    "    print(f\"Responsiveness summary saved to: {output_file}\")\n",
    "\n",
    "\n",
    "# Compute responsiveness for each stress test\n",
    "compute_responsiveness_stress_test(projectPath, mode_dir_1, output_name_1)\n",
    "compute_responsiveness_stress_test(projectPath, mode_dir_2, output_name_2)\n",
    "compute_responsiveness_stress_test(projectPath, mode_dir_3, output_name_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb0901",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231d3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickups_dropoffs(\n",
    "        sf_rides_stats_path,\n",
    "        start_date_str,\n",
    "        start_time_str,\n",
    "        end_date_str,\n",
    "        end_time_str,\n",
    "        tazs_involved = None\n",
    "    ):\n",
    "    start_date = datetime.strptime(start_date_str, \"%y%m%d\").date()\n",
    "    end_date = datetime.strptime(end_date_str, \"%y%m%d\").date()\n",
    "    num_days = (end_date - start_date).days + 1\n",
    "    start_hour = int(datetime.strptime(start_time_str, \"%H\").hour)\n",
    "    end_hour = int(datetime.strptime(end_time_str, \"%H\").hour)\n",
    "\n",
    "    # Map dataset hours (3–26) to standard 0–23 format\n",
    "    dataset_hour_map = {h: h % 24 for h in range(3, 27)}\n",
    "\n",
    "    all_rows = []\n",
    "    with open(sf_rides_stats_path, mode='r') as file:\n",
    "        reader = csv.DictReader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            row['taz'] = int(row['taz'])\n",
    "            row['day_of_week'] = int(row['day_of_week'])\n",
    "            row['hour'] = int(row['hour'])\n",
    "            row['pickups'] = round(float(row['pickups']))\n",
    "            row['dropoffs'] = round(float(row['dropoffs']))\n",
    "            all_rows.append(row)\n",
    "\n",
    "    # Index by (day_of_week, hour, taz)\n",
    "    data_by_key = {}\n",
    "    for row in all_rows:\n",
    "        key = (row['day_of_week'], row['hour'], row['taz'])\n",
    "        data_by_key[key] = {'pickups': row['pickups'], 'dropoffs': row['dropoffs']}\n",
    "\n",
    "    zone_data = {}\n",
    "    # For each simulation day, determine hours to include from that day\n",
    "    for sim_day_index in range(num_days):\n",
    "        sim_date = start_date + timedelta(days=sim_day_index)\n",
    "        sim_day_of_week = sim_date.weekday()\n",
    "        if num_days == 1:\n",
    "            selected_std_hours = list(range(start_hour, end_hour))\n",
    "        else:\n",
    "            if sim_day_index == 0:\n",
    "                selected_std_hours = list(range(start_hour, 24))\n",
    "            elif sim_day_index == num_days - 1:\n",
    "                selected_std_hours = list(range(0, end_hour))\n",
    "            else:\n",
    "                selected_std_hours = list(range(0, 24))\n",
    "        selected_dataset_hours = {h: std for h, std in dataset_hour_map.items() if std in selected_std_hours}\n",
    "        # Filter rows for this day and hour\n",
    "        for row in all_rows:\n",
    "            taz = row['taz']\n",
    "            hour = row['hour']\n",
    "            day = row['day_of_week']\n",
    "            if day == sim_day_of_week and hour in selected_dataset_hours:\n",
    "                std_hour = selected_dataset_hours[hour]\n",
    "                if taz not in zone_data:\n",
    "                    zone_data[taz] = {}\n",
    "                zone_data[taz][std_hour] = {\n",
    "                    'pickups': row['pickups'],\n",
    "                    'dropoffs': row['dropoffs']\n",
    "                }\n",
    "    \n",
    "    # If tazs_involved is provided, adjust pickups and dropoffs based on stress test conditions\n",
    "    for taz in zone_data:\n",
    "        if tazs_involved is None or taz in tazs_involved:\n",
    "            for hour in zone_data[taz]:\n",
    "                zone_data[taz][hour]['pickups'] = round(zone_data[taz][hour]['pickups'] * 1.5)\n",
    "                zone_data[taz][hour]['dropoffs'] = round(zone_data[taz][hour]['dropoffs'] * 1.5)\n",
    "\n",
    "    # Compute pickups and dropoffs across all zones and selected hours\n",
    "    total_pickups = sum(hour_data['pickups'] for zone in zone_data.values() for hour_data in zone.values())\n",
    "    total_dropoffs = sum(hour_data['dropoffs'] for zone in zone_data.values() for hour_data in zone.values())\n",
    "\n",
    "    return total_pickups, total_dropoffs\n",
    "\n",
    "\n",
    "def percent_error(true_val, estimated_val):\n",
    "    return 100 * abs(true_val - estimated_val) / true_val if true_val != 0 else float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c60ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/results/fidelity_stress_1.csv\n",
      "Fidelity summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/results/fidelity_stress_2.csv\n",
      "Fidelity summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/results/fidelity_stress_3.csv\n"
     ]
    }
   ],
   "source": [
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir_1 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_1/social_groups')\n",
    "mode_dir_2 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_2/social_groups')\n",
    "mode_dir_3 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_3/social_groups')\n",
    "output_name_1 = 'fidelity_stress_1'\n",
    "output_name_2 = 'fidelity_stress_2'\n",
    "output_name_3 = 'fidelity_stress_3'\n",
    "\n",
    "def compute_geh(observed, modeled):\n",
    "    return np.sqrt(2 * (observed - modeled)**2 / (observed + modeled + 1e-6))\n",
    "\n",
    "def compute_fidelity_stress_test(projectPath, mode_dir, output_name):\n",
    "    traffic_dir = os.path.join(projectPath, 'data/sf_traffic/sfmta_dataset')\n",
    "    sfcta_dir = os.path.join(projectPath, 'data/ridehailing_stats')\n",
    "    # Initialize records and TAZs involved\n",
    "    records = []\n",
    "    tazs_involved = None\n",
    "    with open(os.path.join(projectPath, \"config/zip_zones_config.json\"), \"r\") as f:\n",
    "        zip_zones = json.load(f)\n",
    "        tazs_involved = []\n",
    "        with open(os.path.join(projectPath, \"data/sf_zones/sf_sfcta_stanford_mapping.json\"), \"r\") as f:\n",
    "            sfcta_mapping = json.load(f)\n",
    "        if output_name == 'fidelity_stress_1':\n",
    "            for taz in zip_zones[\"downtown\"]:\n",
    "                if taz in sfcta_mapping:\n",
    "                    tazs_involved.extend(sfcta_mapping[taz])\n",
    "        elif output_name == 'fidelity_stress_2':\n",
    "            for taz in zip_zones[\"midtown\"]:\n",
    "                if taz in sfcta_mapping:\n",
    "                    tazs_involved.extend(sfcta_mapping[taz])\n",
    "        elif output_name == 'fidelity_stress_3':\n",
    "            tazs_involved = None\n",
    "                    \n",
    "    # Traverse each folder inside the mode\n",
    "    for date in os.listdir(mode_dir):\n",
    "        start, end = date.split('_')\n",
    "        start_date_str = \"20\" + start[:6]\n",
    "        start_time_str = start[6:] + \"00\"\n",
    "        end_date_str = \"20\" + end[:6]\n",
    "        end_time_str = end[6:] + \"00\"\n",
    "        date_path = os.path.join(mode_dir, date)\n",
    "        if not os.path.isdir(date_path):\n",
    "            continue\n",
    "        summary_path = os.path.join(date_path, 'sf_final_metrics.csv')\n",
    "        # Get real traffic data\n",
    "        traffic_file = [f for f in os.listdir(traffic_dir) if os.path.isfile(os.path.join(traffic_dir, f)) and date in f]\n",
    "        try:\n",
    "            traffic_df = pd.read_csv(os.path.join(traffic_dir, traffic_file[0]))\n",
    "            od_df = pd.read_csv(os.path.join(date_path, f\"sf_traffic_od_{date}.csv\"), sep=';')\n",
    "        except:\n",
    "            continue\n",
    "        traffic_df[\"vehicle_position_date_time\"] = pd.to_datetime(traffic_df[\"vehicle_position_date_time\"])\n",
    "        start_dt = datetime.strptime(f\"{start_date_str} {start_time_str}\", \"%Y%m%d %H%M\")\n",
    "        end_dt = datetime.strptime(f\"{end_date_str} {end_time_str}\", \"%Y%m%d %H%M\")\n",
    "        traffic_df = traffic_df[(traffic_df[\"vehicle_position_date_time\"] >= start_dt) & (traffic_df[\"vehicle_position_date_time\"] < end_dt)]\n",
    "        # Filter out vehicles that have speed == 0 for all their records\n",
    "        valid_vehicle_ids = traffic_df.groupby(\"vehicle_id\")[\"average_speed\"].max()\n",
    "        valid_vehicle_ids = valid_vehicle_ids[valid_vehicle_ids > 0].index\n",
    "        traffic_df = traffic_df[traffic_df[\"vehicle_id\"].isin(valid_vehicle_ids)]\n",
    "        # Filter the traffic data by hours and TAZs involved\n",
    "        if tazs_involved:\n",
    "            od_df = od_df[od_df['origin_taz_id'].isin(tazs_involved)]\n",
    "        od_df_filtered_taz = od_df.copy()\n",
    "        od_df_filtered_taz['origin_starting_time'] = pd.to_datetime(od_df_filtered_taz['origin_starting_time'])\n",
    "        time_ranges = [\n",
    "        (pd.to_datetime('8:00:00').time(), pd.to_datetime('11:00:00').time()),\n",
    "        (pd.to_datetime('20:00:00').time(), pd.to_datetime('23:00:00').time())\n",
    "        ]\n",
    "        od_df_filtered_taz_time = od_df_filtered_taz[\n",
    "            od_df_filtered_taz['origin_starting_time'].dt.time.between(*time_ranges[0]) |\n",
    "            od_df_filtered_taz['origin_starting_time'].dt.time.between(*time_ranges[1])\n",
    "        ]\n",
    "        added_count = len(od_df_filtered_taz_time) * 0.5\n",
    "        traffic = (len(traffic_df)-1) if traffic_file else 0\n",
    "        # Scale traffic: 36% of traffic is TNC\n",
    "        traffic_scaled = int((traffic+added_count) * 0.64)\n",
    "        # Get pickups and dropoffs data\n",
    "        start_str, end_str = date.split('_')\n",
    "        pickups, dropoffs = get_pickups_dropoffs(\n",
    "            os.path.join(sfcta_dir, \"trip_stats_taz.csv\"),\n",
    "            start_str[:6],\n",
    "            start_str[6:],\n",
    "            end_str[:6],\n",
    "            end_str[6:],\n",
    "            tazs_involved\n",
    "        )\n",
    "        # Compute errors and record results\n",
    "        if os.path.exists(summary_path):\n",
    "            df = pd.read_csv(summary_path)\n",
    "            if not df.empty:\n",
    "                traffic_count = sum(df['traffic_departures'])\n",
    "                pickup_count = sum(df['passengers_departures'])\n",
    "                dropoff_count = sum(df['passengers_arrivals'])\n",
    "                canceled_count = sum(df['passengers_cancel'])\n",
    "                traffic_error = percent_error(traffic_scaled, traffic_count)\n",
    "                pickup_error = percent_error(pickups, pickup_count)\n",
    "                dropoff_error = percent_error(dropoffs, dropoff_count)\n",
    "                pickup_scaled_error = percent_error(pickups, pickup_count + canceled_count)\n",
    "                dropoff_scaled_error = percent_error(dropoffs, dropoff_count + canceled_count)\n",
    "                # Compute GEH hourly values\n",
    "                if end_time_str in [\"0900\", \"2100\"]:\n",
    "                    geh_traffic = compute_geh(traffic_scaled, traffic_count)\n",
    "                    geh_pickup = compute_geh(pickups, pickup_count + canceled_count)\n",
    "                    geh_dropoff = compute_geh(dropoffs, dropoff_count + canceled_count)\n",
    "                if end_time_str in [\"1100\", \"2300\"]:\n",
    "                    geh_traffic = compute_geh(traffic_scaled/3, traffic_count/3)\n",
    "                    geh_pickup = compute_geh(pickups/3, (pickup_count + canceled_count)/3)\n",
    "                    geh_dropoff = compute_geh(dropoffs/3, (dropoff_count + canceled_count)/3)\n",
    "                if end_time_str in [\"1400\", \"0200\"]:\n",
    "                    geh_traffic = compute_geh(traffic_scaled/6, traffic_count/6)\n",
    "                    geh_pickup = compute_geh(pickups/6, (pickup_count + canceled_count)/6)\n",
    "                    geh_dropoff = compute_geh(dropoffs/6, (dropoff_count + canceled_count)/6)\n",
    "                if end_time_str in [\"2000\", \"0800\"]:\n",
    "                    geh_traffic = compute_geh(traffic_scaled/12, traffic_count/12)\n",
    "                    geh_pickup = compute_geh(pickups/12, (pickup_count + canceled_count)/12)\n",
    "                    geh_dropoff = compute_geh(dropoffs/12, (dropoff_count + canceled_count)/12)\n",
    "                records.append({\n",
    "                    'mode': 'social_groups',\n",
    "                    'date': date,\n",
    "                    'traffic_input': traffic_scaled,\n",
    "                    'pickup_input': pickups,\n",
    "                    'dropoff_input': dropoffs,\n",
    "                    'traffic_output': traffic_count,\n",
    "                    'pickup_output': pickup_count,\n",
    "                    'dropoff_output': dropoff_count,\n",
    "                    'traffic_error': round(traffic_error, 2),\n",
    "                    'pickup_error': round(pickup_error, 2),\n",
    "                    'dropoff_error': round(dropoff_error, 2),\n",
    "                    'canceled_rides': canceled_count,\n",
    "                    'pickup_scaled_error': round(pickup_scaled_error, 2),\n",
    "                    'dropoff_scaled_error': round(dropoff_scaled_error, 2),\n",
    "                    'geh_traffic': round(geh_traffic, 2),\n",
    "                    'geh_pickup': round(geh_pickup, 2),\n",
    "                    'geh_dropoff': round(geh_dropoff, 2)\n",
    "                })\n",
    "\n",
    "    summary_df = pd.DataFrame(records)\n",
    "    output_file = os.path.join(projectPath, f'experiments/results/{output_name}.csv')\n",
    "    summary_df = summary_df.sort_values(by=['date'])\n",
    "    summary_df.to_csv(output_file, index=False)\n",
    "    print(f\"Fidelity summary saved to: {output_file}\")\n",
    "\n",
    "# Compute fidelity for each stress test\n",
    "compute_fidelity_stress_test(projectPath, mode_dir_1, output_name_1)\n",
    "compute_fidelity_stress_test(projectPath, mode_dir_2, output_name_2)\n",
    "compute_fidelity_stress_test(projectPath, mode_dir_3, output_name_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f4ab42",
   "metadata": {},
   "source": [
    "## Responsiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d906a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal responsiveness: 1033.01 seconds\n",
      "Stress test 1 responsiveness: 1444.05 seconds\n",
      "Responsiveness ratio (stress_test_1/normal): 39.79%\n",
      "Stress test 2 responsiveness: 1849.38 seconds\n",
      "Responsiveness ratio (stress_test_2/normal): 79.03%\n",
      "Stress test 3 responsiveness: 2434.29 seconds\n",
      "Responsiveness ratio (stress_test_3/normal): 135.65%\n",
      "\n",
      "Normal agents responsiveness: 47.57%\n",
      "Stress test 1 agents responsiveness: 53.71%\n",
      "Stress test 2 agents responsiveness: 56.13%\n",
      "Stress test 3 agents responsiveness: 59.26%\n",
      "\n",
      "Normal sumo responsiveness: 43.76%\n",
      "Stress test 1 sumo responsiveness: 35.35%\n",
      "Stress test 2 sumo responsiveness: 32.93%\n",
      "Stress test 3 sumo responsiveness: 30.66%\n",
      "\n",
      "Average stress tests agents responsiveness: 56.37%\n",
      "Average stress tests sumo responsiveness: 32.98%\n",
      "Variation of agents responsiveness compared to normal: 18.50%\n",
      "Variation of sumo responsiveness compared to normal: -24.64%\n"
     ]
    }
   ],
   "source": [
    "# Compare responsiveness of stress tests with normal scenario\n",
    "df_stress_1_responsiveness = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_stress_1.csv'))\n",
    "df_stress_2_responsiveness = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_stress_2.csv'))\n",
    "df_stress_3_responsiveness = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_stress_3.csv'))\n",
    "df_normal_responsiveness = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_normal.csv'))\n",
    "df_stress_dates = df_stress_1_responsiveness['date'].unique()\n",
    "df_normal_responsiveness = df_normal_responsiveness[df_normal_responsiveness['mode'] == 'social_groups_avg']\n",
    "df_normal_responsiveness = df_normal_responsiveness[df_normal_responsiveness['date'].isin(df_stress_dates)]\n",
    "normal_seconds = df_normal_responsiveness['elapsed_seconds'].mean()\n",
    "normal_agents_perc = (df_normal_responsiveness['agents_time'].mean()/df_normal_responsiveness['elapsed_seconds'].mean()) * 100\n",
    "normal_sumo_perc = (df_normal_responsiveness['sumo_time'].mean()/df_normal_responsiveness['elapsed_seconds'].mean()) * 100\n",
    "stress_1_seconds = df_stress_1_responsiveness['elapsed_seconds'].mean()\n",
    "stress_1_agents_perc = (df_stress_1_responsiveness['agents_time'].sum()/df_stress_1_responsiveness['elapsed_seconds'].sum()) * 100\n",
    "stress_1_sumo_perc = (df_stress_1_responsiveness['sumo_time'].sum()/df_stress_1_responsiveness['elapsed_seconds'].sum()) * 100\n",
    "stress_2_seconds = df_stress_2_responsiveness['elapsed_seconds'].mean()\n",
    "stress_2_agents_perc = (df_stress_2_responsiveness['agents_time'].sum()/df_stress_2_responsiveness['elapsed_seconds'].sum()) * 100\n",
    "stress_2_sumo_perc = (df_stress_2_responsiveness['sumo_time'].sum()/df_stress_2_responsiveness['elapsed_seconds'].sum()) * 100\n",
    "stress_3_seconds = df_stress_3_responsiveness['elapsed_seconds'].mean()\n",
    "stress_3_agents_perc = (df_stress_3_responsiveness['agents_time'].sum()/df_stress_2_responsiveness['elapsed_seconds'].sum()) * 100\n",
    "stress_3_sumo_perc = (df_stress_3_responsiveness['sumo_time'].sum()/df_stress_3_responsiveness['elapsed_seconds'].sum()) * 100\n",
    "ratio_1 = ((stress_1_seconds - normal_seconds) / normal_seconds) * 100\n",
    "ratio_2 = ((stress_2_seconds - normal_seconds) / normal_seconds) * 100\n",
    "ratio_3 = ((stress_3_seconds - normal_seconds) / normal_seconds) * 100\n",
    "avg_stress_agents_perc = (stress_1_agents_perc + stress_2_agents_perc + stress_3_agents_perc) / 3\n",
    "avg_stress_sumo_perc = (stress_1_sumo_perc + stress_2_sumo_perc + stress_3_sumo_perc) / 3\n",
    "avg_stress_normal_agents_perc = ((avg_stress_agents_perc - normal_agents_perc) / normal_agents_perc) * 100\n",
    "avg_stress_normal_sumo_perc = ((avg_stress_sumo_perc - normal_sumo_perc) / normal_sumo_perc) * 100\n",
    "print(f\"Normal responsiveness: {normal_seconds:.2f} seconds\")\n",
    "print(f\"Stress test 1 responsiveness: {stress_1_seconds:.2f} seconds\")\n",
    "print(f\"Responsiveness ratio (stress_test_1/normal): {ratio_1:.2f}%\")\n",
    "print(f\"Stress test 2 responsiveness: {stress_2_seconds:.2f} seconds\")\n",
    "print(f\"Responsiveness ratio (stress_test_2/normal): {ratio_2:.2f}%\")\n",
    "print(f\"Stress test 3 responsiveness: {stress_3_seconds:.2f} seconds\")\n",
    "print(f\"Responsiveness ratio (stress_test_3/normal): {ratio_3:.2f}%\\n\")\n",
    "print(f\"Normal agents responsiveness: {normal_agents_perc:.2f}%\")\n",
    "print(f\"Stress test 1 agents responsiveness: {stress_1_agents_perc:.2f}%\")\n",
    "print(f\"Stress test 2 agents responsiveness: {stress_2_agents_perc:.2f}%\")\n",
    "print(f\"Stress test 3 agents responsiveness: {stress_3_agents_perc:.2f}%\\n\")\n",
    "print(f\"Normal sumo responsiveness: {normal_sumo_perc:.2f}%\")\n",
    "print(f\"Stress test 1 sumo responsiveness: {stress_1_sumo_perc:.2f}%\")\n",
    "print(f\"Stress test 2 sumo responsiveness: {stress_2_sumo_perc:.2f}%\")\n",
    "print(f\"Stress test 3 sumo responsiveness: {stress_3_sumo_perc:.2f}%\\n\")\n",
    "print(f\"Average stress tests agents responsiveness: {avg_stress_agents_perc:.2f}%\")\n",
    "print(f\"Average stress tests sumo responsiveness: {avg_stress_sumo_perc:.2f}%\")\n",
    "print(f\"Variation of agents responsiveness compared to normal: {avg_stress_normal_agents_perc:.2f}%\")\n",
    "print(f\"Variation of sumo responsiveness compared to normal: {avg_stress_normal_sumo_perc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126e20c",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "466ea71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic accuracy normal: 99.70%\n",
      "Pickup accuracy normal: 99.92%\n",
      "Dropoff accuracy normal: 99.88%\n",
      "Traffic accuracy stress 1: 95.25%\n",
      "Pickup accuracy stress 1: 99.91%\n",
      "Dropoff accuracy stress 1: 99.71%\n",
      "Traffic accuracy stress 2: 92.09%\n",
      "Traffic accuracy stress 3: 84.73%\n",
      "Pickup accuracy stress 2: 99.63%\n",
      "Pickup accuracy stress 3: 99.58%\n",
      "Dropoff accuracy stress 2: 99.31%\n",
      "Dropoff accuracy stress 3: 99.54%\n",
      "\n",
      "Traffic accuracy ratio (stress_test_1/normal): -4.47%\n",
      "Pickup accuracy ratio (stress_test_1/normal): -0.01%\n",
      "Dropoff accuracy ratio (stress_test_1/normal): -0.17%\n",
      "Traffic accuracy ratio (stress_test_2/normal): -7.63%\n",
      "Pickup accuracy ratio (stress_test_2/normal): -0.29%\n",
      "Dropoff accuracy ratio (stress_test_2/normal): -0.57%\n",
      "Traffic accuracy ratio (stress_test_3/normal): -15.02%\n",
      "Pickup accuracy ratio (stress_test_3/normal): -0.34%\n",
      "Dropoff accuracy ratio (stress_test_3/normal): -0.34%\n"
     ]
    }
   ],
   "source": [
    "# Compare fidelity of stress tests with normal scenario\n",
    "df_stress_1_fidelity = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_stress_1.csv'))\n",
    "df_stress_2_fidelity = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_stress_2.csv'))\n",
    "df_stress_3_fidelity = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_stress_3.csv'))\n",
    "df_normal_fidelity = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_normal.csv'))\n",
    "df_stress_dates = df_stress_1_fidelity['date'].unique()\n",
    "df_normal_fidelity = df_normal_fidelity[df_normal_fidelity['mode'] == 'social_groups_avg']\n",
    "df_normal_fidelity = df_normal_fidelity[df_normal_fidelity['date'].isin(df_stress_dates)]\n",
    "normal_traffic_acc = 100 - df_normal_fidelity['traffic_error'].mean()\n",
    "normal_pickup_acc = 100 - df_normal_fidelity['pickup_scaled_error'].mean()\n",
    "normal_dropoff_acc = 100 - df_normal_fidelity['dropoff_scaled_error'].mean()\n",
    "stress_1_traffic_acc = 100 - df_stress_1_fidelity['traffic_error'].mean()\n",
    "stress_2_traffic_acc = 100 - df_stress_2_fidelity['traffic_error'].mean()\n",
    "stress_3_traffic_acc = 100 - df_stress_3_fidelity['traffic_error'].mean()\n",
    "stress_1_pickup_acc = 100 - df_stress_1_fidelity['pickup_scaled_error'].mean()\n",
    "stress_2_pickup_acc = 100 - df_stress_2_fidelity['pickup_scaled_error'].mean()\n",
    "stress_3_pickup_acc = 100 - df_stress_3_fidelity['pickup_scaled_error'].mean()\n",
    "stress_1_dropoff_acc = 100 - df_stress_1_fidelity['dropoff_scaled_error'].mean()\n",
    "stress_2_dropoff_acc = 100 - df_stress_2_fidelity['dropoff_scaled_error'].mean()\n",
    "stress_3_dropoff_acc = 100 - df_stress_3_fidelity['dropoff_scaled_error'].mean()\n",
    "traffic_ratio_1 = ((stress_1_traffic_acc - normal_traffic_acc) / normal_traffic_acc) * 100\n",
    "pickup_ratio_1 = ((stress_1_pickup_acc - normal_pickup_acc) / normal_pickup_acc) * 100\n",
    "dropoff_ratio_1 = ((stress_1_dropoff_acc - normal_dropoff_acc) / normal_dropoff_acc) * 100\n",
    "traffic_ratio_2 = ((stress_2_traffic_acc - normal_traffic_acc) / normal_traffic_acc) * 100\n",
    "pickup_ratio_2 = ((stress_2_pickup_acc - normal_pickup_acc) / normal_pickup_acc) * 100\n",
    "dropoff_ratio_2 = ((stress_2_dropoff_acc - normal_dropoff_acc) / normal_dropoff_acc) * 100\n",
    "traffic_ratio_3 = ((stress_3_traffic_acc - normal_traffic_acc) / normal_traffic_acc) * 100\n",
    "pickup_ratio_3 = ((stress_3_pickup_acc - normal_pickup_acc) / normal_pickup_acc) * 100\n",
    "dropoff_ratio_3 = ((stress_3_dropoff_acc - normal_dropoff_acc) / normal_dropoff_acc) * 100\n",
    "print(f\"Traffic accuracy normal: {normal_traffic_acc:.2f}%\")\n",
    "print(f\"Pickup accuracy normal: {normal_pickup_acc:.2f}%\")\n",
    "print(f\"Dropoff accuracy normal: {normal_dropoff_acc:.2f}%\")\n",
    "print(f\"Traffic accuracy stress 1: {stress_1_traffic_acc:.2f}%\")\n",
    "print(f\"Pickup accuracy stress 1: {stress_1_pickup_acc:.2f}%\")\n",
    "print(f\"Dropoff accuracy stress 1: {stress_1_dropoff_acc:.2f}%\")\n",
    "print(f\"Traffic accuracy stress 2: {stress_2_traffic_acc:.2f}%\")\n",
    "print(f\"Traffic accuracy stress 3: {stress_3_traffic_acc:.2f}%\")\n",
    "print(f\"Pickup accuracy stress 2: {stress_2_pickup_acc:.2f}%\")\n",
    "print(f\"Pickup accuracy stress 3: {stress_3_pickup_acc:.2f}%\")\n",
    "print(f\"Dropoff accuracy stress 2: {stress_2_dropoff_acc:.2f}%\")\n",
    "print(f\"Dropoff accuracy stress 3: {stress_3_dropoff_acc:.2f}%\\n\")\n",
    "print(f\"Traffic accuracy ratio (stress_test_1/normal): {traffic_ratio_1:.2f}%\")\n",
    "print(f\"Pickup accuracy ratio (stress_test_1/normal): {pickup_ratio_1:.2f}%\")\n",
    "print(f\"Dropoff accuracy ratio (stress_test_1/normal): {dropoff_ratio_1:.2f}%\")\n",
    "print(f\"Traffic accuracy ratio (stress_test_2/normal): {traffic_ratio_2:.2f}%\")\n",
    "print(f\"Pickup accuracy ratio (stress_test_2/normal): {pickup_ratio_2:.2f}%\")\n",
    "print(f\"Dropoff accuracy ratio (stress_test_2/normal): {dropoff_ratio_2:.2f}%\")\n",
    "print(f\"Traffic accuracy ratio (stress_test_3/normal): {traffic_ratio_3:.2f}%\")\n",
    "print(f\"Pickup accuracy ratio (stress_test_3/normal): {pickup_ratio_3:.2f}%\")\n",
    "print(f\"Dropoff accuracy ratio (stress_test_3/normal): {dropoff_ratio_3:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
