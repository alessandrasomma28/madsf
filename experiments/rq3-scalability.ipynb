{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055e4dc",
   "metadata": {},
   "source": [
    "# Stress Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6cc30",
   "metadata": {},
   "source": [
    "## Responsiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir_1 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_1/social_groups')\n",
    "mode_dir_2 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_2/social_groups')\n",
    "mode_dir_3 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_3/social_groups')\n",
    "output_name_1 = 'responsiveness_stress_1'\n",
    "output_name_2 = 'responsiveness_stress_2'\n",
    "output_name_3 = 'responsiveness_stress_3'\n",
    "\n",
    "def compute_responsiveness_stress_test(projectPath, mode_dir, output_name):\n",
    "    records = []\n",
    "    # Traverse each folder inside the mode\n",
    "    for date in os.listdir(mode_dir):\n",
    "        date_path = os.path.join(mode_dir, date)\n",
    "        if not os.path.isdir(date_path):\n",
    "            continue\n",
    "        summary_path = os.path.join(date_path, 'simulation_summary.csv')\n",
    "        if os.path.exists(summary_path):\n",
    "            df_summary = pd.read_csv(summary_path)\n",
    "            if not df_summary.empty:\n",
    "                # Get time metrics\n",
    "                first_row = df_summary.iloc[0]\n",
    "                elapsed_sec = first_row['total_elapsed_seconds']\n",
    "                sumo_sec = first_row['sumo_time']\n",
    "                agents_sec = first_row['agents_time']\n",
    "                metrics_path = os.path.join(date_path, 'sf_final_metrics.csv')\n",
    "                if os.path.exists(metrics_path):\n",
    "                    df_metrics = pd.read_csv(metrics_path)\n",
    "                    if not df_metrics.empty:\n",
    "                        # Get traffic, pickups and dropoffs counts\n",
    "                        traffic_count = sum(df_metrics['traffic_departures'])\n",
    "                        pickup_count = sum(df_metrics['passengers_departures'])\n",
    "                        dropoff_count = sum(df_metrics['passengers_arrivals'])\n",
    "                        canceled_count = sum(df_metrics['passengers_cancel']) + sum(df_metrics['rides_not_served'])\n",
    "                        total_load = traffic_count + pickup_count + dropoff_count + canceled_count\n",
    "                        records.append({\n",
    "                            'mode': 'social_groups',\n",
    "                            'date': date,\n",
    "                            'elapsed_seconds': elapsed_sec,\n",
    "                            'elapsed_minutes': round(elapsed_sec / 60, 2),\n",
    "                            'sumo_time': sumo_sec,\n",
    "                            'sumo_minutes': round(sumo_sec / 60, 2),\n",
    "                            'agents_time': agents_sec,\n",
    "                            'agents_minutes': round(agents_sec / 60, 2),\n",
    "                            'sumo_perc_time': round(sumo_sec / elapsed_sec, 2),\n",
    "                            'agents_perc_time': round(agents_sec / elapsed_sec, 2) if agents_sec > 0 else 0,\n",
    "                            'total_load': total_load\n",
    "                        })\n",
    "                        \n",
    "    summary_df = pd.DataFrame(records)\n",
    "    output_file = os.path.join(projectPath, f'experiments/results/{output_name}.csv')\n",
    "    summary_df = summary_df.sort_values(by=['date'])\n",
    "    summary_df.to_csv(output_file, index=False)\n",
    "    print(f\"Responsiveness summary saved to: {output_file}\")\n",
    "\n",
    "\n",
    "# Compute responsiveness for each stress test\n",
    "compute_responsiveness_stress_test(projectPath, mode_dir_1, output_name_1)\n",
    "compute_responsiveness_stress_test(projectPath, mode_dir_2, output_name_2)\n",
    "compute_responsiveness_stress_test(projectPath, mode_dir_3, output_name_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb0901",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickups_dropoffs(\n",
    "        sf_rides_stats_path,\n",
    "        start_date_str,\n",
    "        start_time_str,\n",
    "        end_date_str,\n",
    "        end_time_str,\n",
    "        tazs_involved = None\n",
    "    ):\n",
    "    start_date = datetime.strptime(start_date_str, \"%y%m%d\").date()\n",
    "    end_date = datetime.strptime(end_date_str, \"%y%m%d\").date()\n",
    "    num_days = (end_date - start_date).days + 1\n",
    "\n",
    "    # Parse start and end hours\n",
    "    start_hour = int(datetime.strptime(start_time_str, \"%H\").hour)\n",
    "    end_hour = int(datetime.strptime(end_time_str, \"%H\").hour)\n",
    "\n",
    "    # Map dataset hours (3–26) to standard 0–23 format\n",
    "    dataset_hour_map = {h: h % 24 for h in range(3, 27)}\n",
    "\n",
    "    # Read the CSV file    \n",
    "    all_rows = []\n",
    "    with open(sf_rides_stats_path, mode='r') as file:\n",
    "        reader = csv.DictReader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            row['taz'] = int(row['taz'])\n",
    "            row['day_of_week'] = int(row['day_of_week'])\n",
    "            row['hour'] = int(row['hour'])\n",
    "            row['pickups'] = round(float(row['pickups']))\n",
    "            row['dropoffs'] = round(float(row['dropoffs']))\n",
    "            all_rows.append(row)\n",
    "\n",
    "    # Index by (day_of_week, hour, taz)\n",
    "    data_by_key = {}\n",
    "    for row in all_rows:\n",
    "        key = (row['day_of_week'], row['hour'], row['taz'])\n",
    "        data_by_key[key] = {'pickups': row['pickups'], 'dropoffs': row['dropoffs']}\n",
    "\n",
    "    zone_data = {}\n",
    "    # For each simulation day, determine hours to include from that day\n",
    "    for sim_day_index in range(num_days):\n",
    "        sim_date = start_date + timedelta(days=sim_day_index)\n",
    "        sim_day_of_week = sim_date.weekday()\n",
    "        if num_days == 1:\n",
    "            selected_std_hours = list(range(start_hour, end_hour))\n",
    "        else:\n",
    "            if sim_day_index == 0:\n",
    "                selected_std_hours = list(range(start_hour, 24))\n",
    "            elif sim_day_index == num_days - 1:\n",
    "                selected_std_hours = list(range(0, end_hour))\n",
    "            else:\n",
    "                selected_std_hours = list(range(0, 24))\n",
    "        selected_dataset_hours = {h: std for h, std in dataset_hour_map.items() if std in selected_std_hours}\n",
    "        # Filter rows for this day and hour\n",
    "        for row in all_rows:\n",
    "            taz = row['taz']\n",
    "            hour = row['hour']\n",
    "            day = row['day_of_week']\n",
    "            if day == sim_day_of_week and hour in selected_dataset_hours:\n",
    "                std_hour = selected_dataset_hours[hour]\n",
    "                if taz not in zone_data:\n",
    "                    zone_data[taz] = {}\n",
    "                zone_data[taz][std_hour] = {\n",
    "                    'pickups': row['pickups'],\n",
    "                    'dropoffs': row['dropoffs']\n",
    "                }\n",
    "    \n",
    "    # If tazs_involved is provided, adjust pickups and dropoffs based on stress test conditions\n",
    "    for taz in zone_data:\n",
    "        if tazs_involved is None or taz in tazs_involved:\n",
    "            for hour in zone_data[taz]:\n",
    "                if hour == 23 or hour == 11:\n",
    "                    zone_data[taz][hour]['pickups'] = round(zone_data[taz][hour]['pickups'] * 2)\n",
    "                    zone_data[taz][hour]['dropoffs'] = round(zone_data[taz][hour]['dropoffs'] * 2)\n",
    "\n",
    "    # Compute pickups and dropoffs across all zones and selected hours\n",
    "    total_pickups = sum(hour_data['pickups'] for zone in zone_data.values() for hour_data in zone.values())\n",
    "    total_dropoffs = sum(hour_data['dropoffs'] for zone in zone_data.values() for hour_data in zone.values())\n",
    "\n",
    "    return total_pickups, total_dropoffs\n",
    "\n",
    "\n",
    "def percent_error(true_val, estimated_val):\n",
    "    return 100 * abs(true_val - estimated_val) / true_val if true_val != 0 else float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c60ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare paths\n",
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "mode_dir_1 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_1/social_groups')\n",
    "mode_dir_2 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_2/social_groups')\n",
    "mode_dir_3 = os.path.join(projectPath, 'sumoenv/scenarios/stress_test_3/social_groups')\n",
    "output_name_1 = 'fidelity_stress_1'\n",
    "output_name_2 = 'fidelity_stress_2'\n",
    "output_name_3 = 'fidelity_stress_3'\n",
    "\n",
    "\n",
    "def compute_fidelity_stress_test(projectPath, mode_dir, output_name):\n",
    "    traffic_dir = os.path.join(projectPath, 'data/sf_traffic/sfmta_dataset')\n",
    "    sfcta_dir = os.path.join(projectPath, 'data/ridehailing_stats')\n",
    "    # Initialize records and TAZs involved\n",
    "    records = []\n",
    "    tazs_involved = None\n",
    "    with open(os.path.join(projectPath, \"config/zip_zones_config.json\"), \"r\") as f:\n",
    "        zip_zones = json.load(f)\n",
    "        tazs_involved = []\n",
    "        with open(os.path.join(projectPath, \"data/sf_zones/sf_sfcta_stanford_mapping.json\"), \"r\") as f:\n",
    "            sfcta_mapping = json.load(f)\n",
    "        if output_name == 'fidelity_stress_1':\n",
    "            for taz in zip_zones[\"downtown\"]:\n",
    "                if taz in sfcta_mapping:\n",
    "                    tazs_involved.extend(sfcta_mapping[taz])\n",
    "        elif output_name == 'fidelity_stress_2':\n",
    "            for taz in zip_zones[\"midtown\"]:\n",
    "                if taz in sfcta_mapping:\n",
    "                    tazs_involved.extend(sfcta_mapping[taz])\n",
    "                    \n",
    "    # Traverse each folder inside the mode\n",
    "    for date in os.listdir(mode_dir):\n",
    "        date_path = os.path.join(mode_dir, date)\n",
    "        if not os.path.isdir(date_path):\n",
    "            continue\n",
    "        summary_path = os.path.join(date_path, 'sf_final_metrics.csv')\n",
    "        # Get real traffic data\n",
    "        traffic_file = [f for f in os.listdir(traffic_dir) if os.path.isfile(os.path.join(traffic_dir, f)) and date in f]\n",
    "        try:\n",
    "            traffic_df = pd.read_csv(os.path.join(traffic_dir, traffic_file[0]))\n",
    "            od_df = pd.read_csv(os.path.join(date_path, f\"sf_traffic_od_{date}.csv\"), sep=';')\n",
    "        except:\n",
    "            continue\n",
    "        # Filter the traffic data by hours and TAZs involved\n",
    "        if tazs_involved:\n",
    "            od_df = od_df[od_df['origin_taz_id'].isin(tazs_involved)]\n",
    "        od_df_filtered_taz = od_df.copy()\n",
    "        od_df_filtered_taz['origin_starting_time'] = pd.to_datetime(od_df_filtered_taz['origin_starting_time'])\n",
    "        time_ranges = [\n",
    "        (pd.to_datetime('11:00:00').time(), pd.to_datetime('11:59:59').time()),\n",
    "        (pd.to_datetime('23:00:00').time(), pd.to_datetime('23:59:59').time())\n",
    "        ]\n",
    "        od_df_filtered_taz_time = od_df_filtered_taz[\n",
    "            od_df_filtered_taz['origin_starting_time'].dt.time.between(*time_ranges[0]) |\n",
    "            od_df_filtered_taz['origin_starting_time'].dt.time.between(*time_ranges[1])\n",
    "        ]\n",
    "        base_count = len(od_df_filtered_taz_time) * 0.64\n",
    "        new_count = base_count*2\n",
    "        traffic = (len(traffic_df)-1) if traffic_file else 0\n",
    "        # Scale traffic: 36% of traffic is TNC\n",
    "        traffic = traffic + new_count\n",
    "        traffic_scaled = int(traffic * 0.64)\n",
    "        # Get pickups and dropoffs data\n",
    "        start_str, end_str = date.split('_')\n",
    "        pickups, dropoffs = get_pickups_dropoffs(\n",
    "            os.path.join(sfcta_dir, \"trip_stats_taz.csv\"),\n",
    "            start_str[:6],\n",
    "            start_str[6:],\n",
    "            end_str[:6],\n",
    "            end_str[6:],\n",
    "            tazs_involved\n",
    "        )\n",
    "        # Compute errors and record results\n",
    "        if os.path.exists(summary_path):\n",
    "            df = pd.read_csv(summary_path)\n",
    "            if not df.empty:\n",
    "                traffic_count = sum(df['traffic_departures'])\n",
    "                pickup_count = sum(df['passengers_departures'])\n",
    "                dropoff_count = sum(df['passengers_arrivals'])\n",
    "                canceled_count = sum(df['passengers_cancel'])\n",
    "                traffic_error = percent_error(traffic_scaled, traffic_count)\n",
    "                pickup_error = percent_error(pickups, pickup_count)\n",
    "                dropoff_error = percent_error(dropoffs, dropoff_count)\n",
    "                pickup_scaled_error = percent_error(pickups, pickup_count + canceled_count)\n",
    "                dropoff_scaled_error = percent_error(dropoffs, dropoff_count + canceled_count)\n",
    "                records.append({\n",
    "                    'mode': 'social_groups',\n",
    "                    'date': date,\n",
    "                    'traffic_input': traffic_scaled,\n",
    "                    'pickup_input': pickups,\n",
    "                    'dropoff_input': dropoffs,\n",
    "                    'traffic_output': traffic_count,\n",
    "                    'pickup_output': pickup_count,\n",
    "                    'dropoff_output': dropoff_count,\n",
    "                    'traffic_error': round(traffic_error, 2),\n",
    "                    'pickup_error': round(pickup_error, 2),\n",
    "                    'dropoff_error': round(dropoff_error, 2),\n",
    "                    'canceled_rides': canceled_count,\n",
    "                    'pickup_scaled_error': round(pickup_scaled_error, 2),\n",
    "                    'dropoff_scaled_error': round(dropoff_scaled_error, 2)\n",
    "                })\n",
    "\n",
    "    summary_df = pd.DataFrame(records)\n",
    "    output_file = os.path.join(projectPath, f'experiments/results/{output_name}.csv')\n",
    "    summary_df = summary_df.sort_values(by=['date'])\n",
    "    summary_df.to_csv(output_file, index=False)\n",
    "    print(f\"Fidelity summary saved to: {output_file}\")\n",
    "\n",
    "# Compute fidelity for each stress test\n",
    "compute_fidelity_stress_test(projectPath, mode_dir_1, output_name_1)\n",
    "compute_fidelity_stress_test(projectPath, mode_dir_2, output_name_2)\n",
    "compute_fidelity_stress_test(projectPath, mode_dir_3, output_name_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f4ab42",
   "metadata": {},
   "source": [
    "## Responsiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare responsiveness of stress tests with normal scenario\n",
    "df_stress_1_responsiveness = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_stress_1.csv'))\n",
    "df_stress_2_responsiveness = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_stress_2.csv'))\n",
    "df_stress_3_responsiveness = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_stress_3.csv'))\n",
    "df_normal_responsiveness = pd.read_csv(os.path.join(projectPath, 'experiments/results/responsiveness_normal.csv'))\n",
    "df_stress_dates = df_stress_1_responsiveness['date'].unique()\n",
    "df_normal_responsiveness = df_normal_responsiveness[df_normal_responsiveness['mode'] == 'social_groups_avg']\n",
    "df_normal_responsiveness = df_normal_responsiveness[df_normal_responsiveness['date'].isin(df_stress_dates)]\n",
    "df_normal_stress_ratio = (df_normal_responsiveness['total_load'] / df_normal_responsiveness['elapsed_seconds']).mean()\n",
    "df_stress_1_ratio = (df_stress_1_responsiveness['total_load'] / df_stress_1_responsiveness['elapsed_seconds']).mean()\n",
    "df_stress_2_ratio = (df_stress_2_responsiveness['total_load'] / df_stress_2_responsiveness['elapsed_seconds']).mean()\n",
    "df_stress_3_ratio = (df_stress_3_responsiveness['total_load'] / df_stress_3_responsiveness['elapsed_seconds']).mean()\n",
    "print(f\"Normal responsiveness stress ratio: {df_normal_stress_ratio}\")\n",
    "print(f\"Stress test 1 responsiveness stress ratio: {df_stress_1_ratio}\")\n",
    "print(f\"Stress test 2 responsiveness stress ratio: {df_stress_2_ratio}\")\n",
    "print(f\"Stress test 3 responsiveness stress ratio: {df_stress_3_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126e20c",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ea71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fidelity of stress tests with normal scenario\n",
    "df_stress_1_fidelity = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_stress_1.csv'))\n",
    "df_stress_2_fidelity = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_stress_2.csv'))\n",
    "df_stress_3_fidelity = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_stress_3.csv'))\n",
    "df_normal_fidelity = pd.read_csv(os.path.join(projectPath, 'experiments/results/fidelity_normal.csv'))\n",
    "df_stress_dates = df_stress_1_fidelity['date'].unique()\n",
    "df_normal_fidelity = df_normal_fidelity[df_normal_fidelity['mode'] == 'social_groups_avg']\n",
    "df_normal_fidelity = df_normal_fidelity[df_normal_fidelity['date'].isin(df_stress_dates)]\n",
    "stress_1_traffic_error = df_stress_1_fidelity['traffic_error'].mean()\n",
    "stress_2_traffic_error = df_stress_2_fidelity['traffic_error'].mean()\n",
    "stress_3_traffic_error = df_stress_3_fidelity['traffic_error'].mean()\n",
    "normal_traffic_error = df_normal_fidelity['traffic_error'].mean()\n",
    "stress_1_pickup_error = 100 - df_stress_1_fidelity['pickup_scaled_error'].mean()\n",
    "stress_2_pickup_error = 100 - df_stress_2_fidelity['pickup_scaled_error'].mean()\n",
    "stress_3_pickup_error = 100 - df_stress_3_fidelity['pickup_scaled_error'].mean()\n",
    "normal_pickup_error = 100 - df_normal_fidelity['pickup_scaled_error'].mean()\n",
    "stress_1_dropoff_error = 100 - df_stress_1_fidelity['dropoff_scaled_error'].mean()\n",
    "stress_2_dropoff_error = 100 - df_stress_2_fidelity['dropoff_scaled_error'].mean()\n",
    "stress_3_dropoff_error = 100 - df_stress_3_fidelity['dropoff_scaled_error'].mean()\n",
    "normal_dropoff_error = 100 - df_normal_fidelity['dropoff_scaled_error'].mean()\n",
    "traffic_ratio_1 = ((stress_1_traffic_error - normal_traffic_error) / normal_traffic_error) * 100\n",
    "traffic_ratio_2 = ((stress_2_traffic_error - normal_traffic_error) / normal_traffic_error) * 100\n",
    "traffic_ratio_3 = ((stress_3_traffic_error - normal_traffic_error) / normal_traffic_error) * 100\n",
    "pickup_ratio_1 = ((stress_1_pickup_error - normal_pickup_error) / normal_pickup_error) * 100\n",
    "pickup_ratio_2 = ((stress_2_pickup_error - normal_pickup_error) / normal_pickup_error) * 100\n",
    "pickup_ratio_3 = ((stress_3_pickup_error - normal_pickup_error) / normal_pickup_error) * 100\n",
    "dropoff_ratio_1 = ((stress_1_dropoff_error - normal_dropoff_error) / normal_dropoff_error) * 100\n",
    "dropoff_ratio_2 = ((stress_2_dropoff_error - normal_dropoff_error) / normal_dropoff_error) * 100\n",
    "dropoff_ratio_3 = ((stress_3_dropoff_error - normal_dropoff_error) / normal_dropoff_error) * 100\n",
    "print(f\"Traffic fidelity ratio (stress 1/normal): {traffic_ratio_1:.2f}%\")\n",
    "print(f\"Traffic fidelity ratio (stress 2/normal): {traffic_ratio_2:.2f}%\")\n",
    "print(f\"Traffic fidelity ratio (stress 3/normal): {traffic_ratio_3:.2f}%\")\n",
    "print(f\"Pickup fidelity ratio (stress 1/normal): {pickup_ratio_1:.2f}%\")\n",
    "print(f\"Pickup fidelity ratio (stress 2/normal): {pickup_ratio_2:.2f}%\")\n",
    "print(f\"Pickup fidelity ratio (stress 3/normal): {pickup_ratio_3:.2f}%\")\n",
    "print(f\"Dropoff fidelity ratio (stress 1/normal): {dropoff_ratio_1:.2f}%\")\n",
    "print(f\"Dropoff fidelity ratio (stress 2/normal): {dropoff_ratio_2:.2f}%\")\n",
    "print(f\"Dropoff fidelity ratio (stress 3/normal): {dropoff_ratio_3:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
