{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99447a3b",
   "metadata": {},
   "source": [
    "# RQ2: Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "46866435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b5ce93",
   "metadata": {},
   "source": [
    "## Compute Fidelity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a826424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickups_dropoffs(\n",
    "        sf_rides_stats_path,\n",
    "        start_date_str,\n",
    "        start_time_str,\n",
    "        end_date_str,\n",
    "        end_time_str\n",
    "    ):\n",
    "    start_date = datetime.strptime(start_date_str, \"%y%m%d\").date()\n",
    "    end_date = datetime.strptime(end_date_str, \"%y%m%d\").date()\n",
    "    num_days = (end_date - start_date).days + 1\n",
    "\n",
    "    # Parse start and end hours\n",
    "    start_hour = int(datetime.strptime(start_time_str, \"%H\").hour)\n",
    "    end_hour = int(datetime.strptime(end_time_str, \"%H\").hour)\n",
    "\n",
    "    # Map dataset hours (3–26) to standard 0–23 format\n",
    "    dataset_hour_map = {h: h % 24 for h in range(3, 27)}\n",
    "\n",
    "    # Read the CSV file    \n",
    "    all_rows = []\n",
    "    with open(sf_rides_stats_path, mode='r') as file:\n",
    "        reader = csv.DictReader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            row['taz'] = int(row['taz'])\n",
    "            row['day_of_week'] = int(row['day_of_week'])\n",
    "            row['hour'] = int(row['hour'])\n",
    "            row['pickups'] = round(float(row['pickups']))\n",
    "            row['dropoffs'] = round(float(row['dropoffs']))\n",
    "            all_rows.append(row)\n",
    "\n",
    "    # Index by (day_of_week, hour, taz)\n",
    "    data_by_key = {}\n",
    "    for row in all_rows:\n",
    "        key = (row['day_of_week'], row['hour'], row['taz'])\n",
    "        data_by_key[key] = {'pickups': row['pickups'], 'dropoffs': row['dropoffs']}\n",
    "\n",
    "    zone_data = {}\n",
    "    # For each simulation day, determine hours to include from that day\n",
    "    for sim_day_index in range(num_days):\n",
    "        sim_date = start_date + timedelta(days=sim_day_index)\n",
    "        sim_day_of_week = sim_date.weekday()\n",
    "        if num_days == 1:\n",
    "            selected_std_hours = list(range(start_hour, end_hour))\n",
    "        else:\n",
    "            if sim_day_index == 0:\n",
    "                selected_std_hours = list(range(start_hour, 24))\n",
    "            elif sim_day_index == num_days - 1:\n",
    "                selected_std_hours = list(range(0, end_hour))\n",
    "            else:\n",
    "                selected_std_hours = list(range(0, 24))\n",
    "        selected_dataset_hours = {h: std for h, std in dataset_hour_map.items() if std in selected_std_hours}\n",
    "        # Filter rows for this day and hour\n",
    "        for row in all_rows:\n",
    "            taz = row['taz']\n",
    "            hour = row['hour']\n",
    "            day = row['day_of_week']\n",
    "            if day == sim_day_of_week and hour in selected_dataset_hours:\n",
    "                std_hour = selected_dataset_hours[hour]\n",
    "                if taz not in zone_data:\n",
    "                    zone_data[taz] = {}\n",
    "                zone_data[taz][std_hour] = {\n",
    "                    'pickups': row['pickups'],\n",
    "                    'dropoffs': row['dropoffs']\n",
    "                }\n",
    "\n",
    "    # Compute pickups and dropoffs across all zones and selected hours\n",
    "    total_pickups = sum(hour_data['pickups'] for zone in zone_data.values() for hour_data in zone.values())\n",
    "    total_dropoffs = sum(hour_data['dropoffs'] for zone in zone_data.values() for hour_data in zone.values())\n",
    "\n",
    "    return total_pickups, total_dropoffs\n",
    "\n",
    "\n",
    "def percent_error(true_val, estimated_val):\n",
    "    return 100 * abs(true_val - estimated_val) / true_val if true_val != 0 else float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b80d7dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213248.599999927 1213248.599999927\n",
      "Total pickups: 1210741\n",
      "Total dropoffs: 1211023\n",
      "Fidelity summary saved to: /Users/beyzaeken/Desktop/sfdigitalmirror/experiments/fidelity_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare paths\n",
    "projectPath = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "modes_dir = os.path.join(projectPath, 'sumoenv/scenarios/normal')\n",
    "traffic_dir = os.path.join(projectPath, 'data/sf_traffic/sfmta_dataset')\n",
    "sfcta_dir = os.path.join(projectPath, 'data/ridehailing_stats')\n",
    "\n",
    "records = []\n",
    "# Traverse each mode (sumo, social_groups, multi_agent)\n",
    "for mode in os.listdir(modes_dir):\n",
    "    mode_path = os.path.join(modes_dir, mode)\n",
    "    if not os.path.isdir(mode_path):\n",
    "        continue\n",
    "    # Traverse each folder inside the mode\n",
    "    for date in os.listdir(mode_path):\n",
    "        date_path = os.path.join(mode_path, date)\n",
    "        if not os.path.isdir(date_path):\n",
    "            continue\n",
    "        summary_path = os.path.join(date_path, 'sf_final_metrics.csv')\n",
    "        # Get real traffic and pickup/dropoff data\n",
    "        traffic_file = [f for f in os.listdir(traffic_dir) if os.path.isfile(os.path.join(traffic_dir, f)) and date in f]\n",
    "        traffic_df = pd.read_csv(os.path.join(traffic_dir, traffic_file[0]))\n",
    "        traffic = (len(traffic_df)-1) if traffic_file else 0\n",
    "        # Scale traffic: 36% of traffic is TNC\n",
    "        traffic_scaled = int(traffic * 0.64)\n",
    "        start_str, end_str = date.split('_')\n",
    "        pickups, dropoffs = get_pickups_dropoffs(\n",
    "            os.path.join(sfcta_dir, \"trip_stats_taz.csv\"),\n",
    "            start_str[:6],\n",
    "            start_str[6:],\n",
    "            end_str[:6],\n",
    "            end_str[6:]\n",
    "        )\n",
    "        if os.path.exists(summary_path):\n",
    "            df = pd.read_csv(summary_path)\n",
    "            if not df.empty:\n",
    "                traffic_count = sum(df['traffic_departures'])\n",
    "                pickup_count = sum(df['passengers_departures'])\n",
    "                dropoff_count = sum(df['passengers_arrivals'])\n",
    "                canceled_count = sum(df['passengers_cancel'])\n",
    "                traffic_error = percent_error(traffic_scaled, traffic_count)\n",
    "                pickup_error = percent_error(pickups, pickup_count)\n",
    "                dropoff_error = percent_error(dropoffs, dropoff_count)\n",
    "                pickup_scaled_error = percent_error(pickups, pickup_count + canceled_count) if canceled_count > 0 else 0\n",
    "                dropoff_scaled_error = percent_error(dropoffs, dropoff_count + canceled_count) if canceled_count > 0 else 0\n",
    "                records.append({\n",
    "                    'mode': mode,\n",
    "                    'date': date,\n",
    "                    'traffic_input': traffic_scaled,\n",
    "                    'pickup_input': pickups,\n",
    "                    'dropoff_input': dropoffs,\n",
    "                    'traffic_output': traffic_count,\n",
    "                    'pickup_output': pickup_count,\n",
    "                    'dropoff_output': dropoff_count,\n",
    "                    'traffic_error': round(traffic_error, 2),\n",
    "                    'pickup_error': round(pickup_error, 2),\n",
    "                    'dropoff_error': round(dropoff_error, 2),\n",
    "                    'canceled_rides': canceled_count,\n",
    "                    'pickup_scaled_error': round(pickup_scaled_error, 2),\n",
    "                    'dropoff_scaled_error': round(dropoff_scaled_error, 2)\n",
    "                })\n",
    "\n",
    "# Save results to CSV\n",
    "summary_df = pd.DataFrame(records)\n",
    "output_file = os.path.join(projectPath, 'experiments/fidelity_results.csv')\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Fidelity summary saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e26295",
   "metadata": {},
   "source": [
    "## Analyze Fidelity Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
